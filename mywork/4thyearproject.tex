
\documentclass[a4paper,10pt]{article}
\usepackage{amsmath,amssymb,amsthm}
\renewcommand{\qedsymbol}{QED}

%opening
\title{4th year Project}
\author{Joe Tait}

\newtheorem{thm}{Theorem}[subsection]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{ex}[thm]{Example}

\begin{document}

\maketitle
\setcounter{section}{0}

\begin{abstract}
The aim of this paper is to give an overview of what K-theory is for $K_{0}$, $K_{1}$ and $K_{2}$ at a level that should be approachable by a fellow fourth year undergraduaate. This should include basic definitions and results as well as examples to enlighten the reader and applications to motivate them. Of course, major results in the field will also be stated and where appropriate proofs will be given.
\end{abstract}

\section{Introduction}

\section{$K_{0}$}

\subsection{The basics}

The elements of $K_{0}$ are isomorphism classes of projectiove modules, which can be formed in to a ring under the appropriate operations. Clearly this makes projective modules a fundamental part of $K_{0}$, so that is the first concept I will look at.

\begin{defn}
	Let $R$ be a ring. Then an $R$-module $P$ is projective if there exists an $R$-module $Q$ such that the direct sum $P\oplus Q$ is free.
\end{defn}

\textbf{NEED TO PUT IN OTHER DEFINITION!!}


The obvious, trivial examples of finitely generated projective mdoules over a ring $R$ are $R^n$ for any $n\in \mathbb{N}$. A simple, non-trivial example (i.e. a module which is projective but not already free) can be found over the ring $\mathbb{Z}_6$. Over this ring any free module must have order $6^n$ for some $n$, meaning that both the subrings $\{0,3\}$ and $\{0,2,4\}$ are not free. Their direct sum is, however, the whole ring, meaning that they are both projective. For the rest of this paper it will be assumed whenever projective modules are mentioned that they are finite (unless otherwise stated).


\begin{thm}
	The isomoprhism classes of projective modules form a semi group $ProjR$, with the group operation being direct sum.
\end{thm}
\begin{proof}
	If $P\cong P'$ and $Q\cong Q'$ then clearly $P\oplus Q\cong P'\oplus Q'$, so addition in the semi group is well defined.
	$<0>$ is an identity, as $0\oplus P\cong P$ $\forall P\in ProjR$.
	\newline If $R$ and $S$ are chosen such that $P\oplus R$ and $Q\oplus S$ are free, then $(P\oplus Q)\oplus (R\oplus S)$ is free, so $ProjR$ is closed.
	\newline Lastly,direct sum is still associative when applied to isomorphism classes.
\end{proof}

Now that we have formed a semi group, we can define $K_0(R)$ to be the group completetion of this.

\begin{defn}
	Let $T$ be the free abelian group generated by the elements of $Proj(R)$, and let $S$ be the subgroup generated by $<P>+<Q>=<P\oplus Q>$, where $<B>$ represents the isomorphism class of the projective module $B$. Then $K_{0}(R)$ is the quotient group $T/S$. A member of $K_{0}(R)$ will be denoted $[P]$.
\end{defn}

\begin{ex}
	Let $F$ be a field. Then all finitely generated submodules of $F$ are finite dimensional vector spaces, which are all free. Take the map $f:F\rightarrow \mathbb{Z}$ mapping projective modules to their dimension. As for any projective modules $P$ and $Q$ we have $dim(P\oplus Q)=dim(P)+dim(Q)$, thenafter taking isomorphism classes the kernel of $f$ is contained in the subgroup generated by $[P]+[Q]-[P\oplus Q]$. As such we can define $\tilde{f}:K_{0}(F)\rightarrow \mathbb{Z}$
\end{ex}

\begin{thm}
	Let $P,Q$ be projective modules over $R$. Then $P\oplus R^{n}\cong Q\oplus R^{n}$ for some n iff $[P]=[Q]$ in $K_{0}(R)$.
\end{thm}
\begin{proof}
	To prove the only if statement,notice $P\oplus R^{n}\cong Q\oplus R^{n} \ \Rightarrow \ [P\oplus R^{n}]=[Q\oplus R^{n}] \ \Rightarrow  \ [P]\oplus [R^{n}]=[Q]\oplus [R^{n}] \ \Rightarrow \ [P]=[Q]$
	\newline To show the other implication, we know that $[P]-[Q]=0$, or, in other words, $<P>-<Q>\in S$. So in $T$ we can write $$<P>-<Q>=\underset{i}{\Sigma}(<A_{i}>+<B_{i}>-<A_{i}\oplus B_{i}>)$$ $$-\underset{j}{\Sigma}(<C_{j}>+<D_{j}>-<C_{j}\oplus D_{j}>)$$. Noting that $$\underset{i}{\Sigma}(<A_{i}\oplus B_{i}>)=\underset{i}{\Sigma}(<A_{i}>+<B_{i}>)$$ and like wise for $<C_{j}>,\ <D_{j}>$ we can rearrange to get $P\oplus M\cong Q\oplus M$, where $M=A\oplus B\oplus C\oplus D$ and $A=\underset{i}{\oplus}A_{i}$, $B=\underset{i}{\oplus}B_{i}$, $C=\underset{j}{\oplus}C_{j}$, $D=\underset{j}{\oplus}D_{j}$. As $M$ is a finite sum of finitely generated projective modules, it is itself a projective module, so there exists a module, call it $N$, such that $M\oplus N\cong R^{n}$, some $N\in \mathbb{N}$, thus completing the proof.
\end{proof}

\begin{ex}
	This is another computation of $K_{0}(R)$, this time to show it is trivial. Let $V$ be an infinite dimensional vector space over $F$ and let $R=End_{F}(V)$. Using the identity $V\oplus V\cong V$ we can see $$R\oplus R=Hom_{F}(V,V)\oplus Hom_{F}(V,V)\cong Hom_{F}(V\oplus V,V)\cong Hom_{F}(V,V)=R$$, so $R^{2}\cong R$. It follows that $R^{n}\cong R \ \forall n\in \mathbb{N}$.
	\newline So in $K_{0}(R)$ $[R]=[R\oplus R]=[R]+[R]$, so $[R^{n}]=[0]$ $\forall n$.
	Now let $P$ be a finitely generated projective R-module. So $\exists Q$ such that $P\oplus Q\cong R^{n}\cong R$. So $P$ and $Q$ are left ideals of $R$. Now as $P\oplus Q\cong R$ $\exists p\in P$ and $q\in Q$ such that $1=p+q$. now if $r\in P$ then $r.1=rp+rq$ and $r-rp=rq\in P\cap Q=0$, and similarly for $s\in Q$. SO $p^{2}=p$, $q^{2}=q$ and $pq=qp=0$. Now by observing that $1.V=(p+q)V$ and applying the relations gained so far we can see that $P\cong Hom_{F}(pV,V)$. Similary, $Q\cong Hom_{F}(qV,V)$.
	Also, $V=pV\oplus qV$, which means that, as $V$ is infite dimensional, at least one of $pV$ or $qV$ is isomorphic to $V$ itself.
	\newline So if $p/v\cong V$ then $P\cong Hom_{F}(pV,V)\cong Hom_{F}(V,V)=R$ and $[P]=[R]=[0]$. If $qV\cong V$ then similarly $[Q]=[0]$ and so $[P]=[P]+[Q]=[P\oplus Q]=[R]$ so $[P]=[0]$ as well and $K_{0}(R)$ is trivial.
\end{ex}




\subsection{Idempotents - another perspective}
	Another, perhaps more concrete and obvious way to understand $K_{0}$ is by showing that there is a one-to-one correspondance between isomorphism classes of projective modules and orbits of idempotent matrices.

\begin{defn}
	A map $p:A\rightarrow B$ is idempotent if $p^2=p$
\end{defn}

Notice that if and idempotent map $p:A\rightarrow B$ is surjective and has a right inverse, say $q$ (as is the case for projective modules), then $f=q\circ p$ is idempotent. So by taking the map $a\mapsto (p(a), f(a))$ gives and isomorphism $B\cong A\oplus (1-f)(B)$

So now we can show how each projective module gives rise to an idemptotent map and then go on to make a corresponence with the isomporhpism classes. For each projective module $P$ there is some projective module $P'$ such that $P\oplus P'\cong R^{n}$ for some $n$. Take the isomorphism that is the identity on both $P$ and $P'$ and restrict this to $P$. Call this restirction $p$. Then $p$ is clearly idempotent, being an identiy or zero map on any element. It is also a homomorphism from $R^{n}\rightarrow R^{n}$ and as such can be represented by a matrix $p$ giving the equality $P=R^{n}p$.

Now note that using the injection from $GL(n;R)\rightarrow GL(n+1;R)$ $a\mapsto \left( \begin{array}{cc} a & 0 \\ 0 & 1 \end{array}\right)$, we can take the inclusions

\begin{center}
	$GL(1;R)\subset GL(2;R)\subset GL(3;R)\dots$
\end{center}

Then $GL(R)$ is defined to be the infinite union of the above sequence. 
\newline $M(R)$ is defined similarly, with the inclusion map being $a\mapsto \left( \begin{array}{cc} a & 0 \\ 0 & 0 \end{array}\right)$

Now we can prove the following theorem
\begin{thm}
	If $p$ and $q$ are idempotent matrices over a ring $R$ then the corresponding finitely generated projective modules, call them $P$ and $Q$, are isomorphic if and only if the embeddings of $p$ and $q$ in $GL(R)$ are conjugate.
\end{thm}
\begin{proof}
	Suppose $\exists u\in GL(N,R)$ such that $up=qu$. Then as $u\in GL(N,R)$ we have $R^Np\cong R^Nup\cong R^Nqu\cong R^Nq$, showing that it is a sufficent condition.
	\newline To prove the necessary part if condition, start by taking an isomorphism $f: R^{n}p\rightarrow R^{m}q$. Then by defining $f'$ to be $f$ on $R^{n}$ and $0$ on $(1-f)R^{n}$ we extend the domain of $f$ to $R^{n}$ and by embedding $R^{n}q$ in $R^{n}$ we can extend the codomain to $R^{n}$.Similarly we can extend the inverse $f^{-1}$ to $f'^{-1}$.$f'$ and $f'^{-1}$ are homorphisms and so can represented by matrics sat $A$ and $B$ of sizes $n\times m$ and $m\times n$ repszectively. Now look at the $((n+m)\times (n+m))$ block matrix:$$\left(\begin{array}{cc} 1-p & A \\ B & 1-q \end{array}\right)$$
	\newline using the following relations: $AB=p$, $BA=q$, $A=pA=Aq$, $B=Bp=qB$, which are easily verified (e.g $R^{m}BA=f'\circ f'^{-1}(R^{m})=R^{n}p$), we can see that both $$\left(\begin{array}{cc} 1-p & A \\ B & 1-q \end{array}\right)^{2}=\left(\begin{array}{cc} 1 & 0 \\ 0 & 1 \end{array}\right)$$ so $$\left(\begin{array}{cc} 1-p & A \\ B & 1-q \end{array}\right)\in GL((m+n);R)\subseteq GL(R)$$ and that
	$$\left(\begin{array}{cc} 1-p & A \\ B & 1-q \end{array}\right)\left(\begin{array}{cc} p & 0 \\ 0 & 0 \end{array}\right)\left(\begin{array}{cc} 1-p & A \\ B & 1-q \end{array}\right)=\left(\begin{array}{cc} 0 & 0 \\ 0 & q \end{array}\right)$$
	Of course $\left(\begin{array}{cc} 0 & 0 \\ 0 & q \end{array}\right)$ is conjugate to $\left(\begin{array}{cc} q & 0 \\ 0 & 0 \end{array}\right)$ and we are done.
\end{proof}

So now we can identify $ProjR$ with the equivelance classes of idempotent matrices under conjugation by $GL(R)$. By saying that if $[P]$ and $[Q]$ in $ProjR$ if are represented respectively by $p$ and $q$ in $GL(R)$, then $P\oplus Q$ is represented by $\left(\begin{array}{cc} p & 0 \\ 0 & q \end{array}\right)$ we have an equivelance between $ProjR$ and the equivelance classes of idempotent matrices as semi groups. It follows that the group closure of the conjugated idempotent matrices is equivalent to $K_{0}(R)$

\begin{lem}[Morita Invariance]
	Let $R$ be a ring. Then $K_{0}(R)\cong K_{0}(M_{n}(R))$
\end{lem}
\begin{proof}
	By using block matrices to identify $M_{k}(M_{n}(R)$ with $M_{kn}(R)$ we see that $Idem(M_{n}(R))=Idem(R)$ and that $GL(M_{n}(R))=GL(R)$. So, using that last theorem we have $K_{0}(R)\cong Idem(R)/\{upu^{-1}|u\in GL(R),\ p\in Idem(R)\}\cong Idem(M_{n}(R))/\{upu^{-1}|u\in GL(M_{n}(R)),\ p\in Idem(M_{n}(R))\}\cong K_{0}(M_{n}(R)$.
\end{proof}

\begin{thm}
	If $R=R_{1}\times R_{2}$ then $K_{0}(R)=K_{0}(R_{1})\oplus K_{0}(R_{2})$
\end{thm}
\begin{proof}
	We can idenfity $Proj(R)$ with $Proj(R_{1})\times Proj(R_{2})$ by taking the isomorphism $$Idem(R)/\{upu^{-1}|u\in GL(R)$$ $$ p\in Idem(R)\}\cong (Idem(R_{1})/\{u_{1}p_{1}{u^{-1}}_{1}|u_{1}\in GL(R_{1})$$ $$p_{1}\in Idem(R_{1})\}\times Idem(R_{2})/\{u_{2}p_{2}{u^{-1}}_{2}|u_{2}\in GL(R_{2})$$ $$p_{2}\in Idem(R_{2})\}$$ Thus $$Proj(R)\cong Proj(R_{1})\times Proj(R_{2})$$ and so $$K_{0}(R)=K_{0}(R_{1})\oplus K_{0}(R_{2})$$
\end{proof}

\begin{cor}
	If $R=R_{1}\times\ldots\times R_{n}$ then $K_{0}(R)=K_{0}(R_{1})\oplus\ldots\oplus K_{0}(R_{n})$
\end{cor}


\textbf{PUT IN EXERCISES 1.2.8 AND 1.2.9}

\subsection{PIDs and local rings}

By restricting what rings we look at, $K_{0}$ can be made much easier to compute. With this in mind, we will now develop K-theory specifically for PIDs and local rings.

\begin{thm}
	If $R$ is a PID then every finitely generated module over $R$ is isomorphic to $R^{n}$ for some n. This $n$ is unique and is called the rank of the module.
\end{thm}
\begin{proof}
	Let $M$ be a finitely generated projective module over $R$. $M$ could be viewed as being embedded in $R^{n}$ for some $n$. If $n=0$ then of course $M=\{0\}$ (as it is non-empty) and is isomorphic to $R^{0}$.
	Assume that for $k<n$ the theorem holds. Let $\pi :R^{n}\rightarrow R$ map the n-th co-ordinate to R. If $\pi (M)=0$ then $M$ can be embedded in $R^{n-1}$ and so by the induction hypothesis we ared one.
	If $\pi (M)\neq0$ then, as $\pi (M)$ is an ideal and $R$ is a PID then $\pi (M)\cong R$. So $M\cong ker\pi |_{M} \oplus R$.As $ker(\pi |_{M}$ is embedded in $R^{n-1}$, by induction hypothesis it is isomorphic to $R^{k'}$, some $k'\leq k$, so $M\cong R^{k'+1}$.
\end{proof}

\begin{cor}
	If $R$ is a PID then every projective module over $R$ is isomorphic to $R^{n}$ for some n. This $n$ is unique and is called the rank of the module.
\end{cor}

Every ring has a unique homorphism from $\mathbb{Z}$ to itself, defined by mapping $1$ to the identity in the ring. Also, as already shown, $K_{0}(\mathbb{Z})\cong \mathbb{Z}$. Therefore we can define a map , $h:\mathbb{Z}\rightarrow K_{0}(R)$.

\begin{defn}
	The reduced $K_{0}$ group of $R$ is $\tilde{K_{0}}(R)\underset{def}{=}K_{0}(R)/h(\mathbb{Z})$
\end{defn}

Note that by corollary 2.3.2, $\tilde{K_{0}}(R$ is trivial if $R$ is a PID.

\subsection{Local Rings}

To start with we shall show that for all local rings R then $K_{0}(R)\cong \mathbb{Z}$; obviously this makes them much easier to deal with.

Before this it will be necessary to look at some basic properties of the rings we will be studying.

\begin{defn}
	A ring $R$ is local if it's non-invertible elements form a proper ideal $M$ of $R$.
\end{defn}

\begin{prop}
	A ring $R$ is local iff it has a unique maximal right ideal, unique maximal left, and these are the same.
\end{prop}
\begin{proof}
	Suppose $R$ is local and $M$ is the ideal formed by non-invertible elements. If there was a left ideal properly containing $M$, then there would some element $a\in M\ R$. This would have to be invertible, so the ideal would be the whole of $R$. Similarly for any right ideal. So $M$ is a unique two-sided maximal ideal.
	\newline to shw the other implication, let $a\in R$. If $a$ has no right inverse $aR$ is a proper right ideal and by Zorn's lemma this is in a maximal right ideal, unique by assumption. Similarly for any $a$ without a left inverse. Therefore all elements with no inverse lie in the unique maximal ideal.
\end{proof}

\begin{defn}
The Jacosbon radical is the intersection of all the maximal right ideals of a ring R. It is denoted by $J(R)$ or just $J$.
\end{defn}

Remark: At this stage this is the right Jacobson radical., though symmetry will be shown later.

\begin{lem}
Let $M$ be a maximal right ideal of a ring $R$. Let $a \in R$. Define $K=\{ x\in R|ax\in M\}$. $K \underset{r}{\vartriangleleft} R$ and
	\begin{enumerate}
		\item if $a\in M$ then $K=R$ \item if $a\notin M$ then $K$ is a maximal right ideal of $R$
	\end{enumerate}
\end{lem}
\begin{proof}
It is clear that $K \underset{r}{\vartriangleleft} R$
	\begin{enumerate}
		\item if $a\in M$ then clearly $1\in K$ therefore $K=R$ \item Assume $a\notin M$. Then $M+aR$ is a right ideal and $M+aR\supsetneqq  M \Rightarrow M+aR=R$. Define an R module homomorphism by $\theta(r)=ar+M \forall r\in R$. $\theta$ is surjective. ALso, $Ker\theta =K$ so by the $1^{st}$ isomorphism theorem for modules $R/M\cong R/Ker\theta =R/K$. It folllos that $K$ is a maximal right ideal.
	\end{enumerate}
\end{proof}
\begin{thm}
 $J$ is an ideal of $R$
\end{thm}

\begin{proof}
$J\underset{r}{\vartriangleleft} R$ is clear. Now let $j\in J$ and $a\in R$. Suppose that $aj\notin J$. Then by definition $\exists$ a maximal right ideal $M$ such that $aj\notin M$ so clearly $a\notin M$. \newline Define $K=\{ r\in R|ar\in M\}$ which is a maximal right ideal by (3). But $j\notin K$ as $aj\notin M$. So $j\notin J$. Contradiction.

Hence $aj\in J$ $\forall a\in R, j\in J$ so $J\underset{l}{\vartriangleleft} R$ and $J\underset{r}{\vartriangleleft} R$. Hence $J\vartriangleleft R$.
\end{proof}

\begin{defn}
Let $x\in R$. We say that x is right quasi-regular (rqr) if 1-x has a right inverse.\newline A subset $S$ of $R$ is said to be rqr if every element of $S$ is rqr. Similarly for lqr. A subset or $R$ is quasi-regular (qr) if it is both lqr and rqr.
\end{defn}

\begin{lem}
 Let $I$ be a rqr right ideal of $R$. Then $I\subseteq JR)$.
\end{lem}
\begin{proof}
Let $M$ be a maximal right ideal of $R$. If $I\nsubseteq M$ then $I+M=R$. So $1=x+m$ for some $x\in I, m\in M$. So $1-x\in M$ but x is rqr hence $\exists y\in R$ such that $(1-x)y=1)$. Hence $M+R$. Contradiction.
\end{proof}

Note that $x\in r$ rqr $\nRightarrow x\in J(R)$

\begin{lem}
 Let $R$ be a ring. Then $J(R)$ is a rqr ideal.
\end{lem}
\begin{proof}
Let $j\in J(R)$. Suppose $(1-j)$ has no right inverse. Then $(1-j)\neq R$. So by Zorns lemma $\exists$ maximal right ideal $M$ such that $(1-j)R\subseteq M\Rightarrow 1-j\in M$ and $j\in M\Rightarrow 1\in M$
\end{proof}

\begin{lem}
Let $I$ be an ideal of a ring $R$. Then $I$ is rqr $\Leftrightarrow$ $I$ is lqr
\end{lem}
\begin{proof}
Suppose $I$ is rqr. Let $x\in I$. Then $\exists a\in R$ such that $(1-x)(1-a)=1$ (Taking $a=1-y$). So $1-x-a+xa=1$ so $a=xa-x$. Hence $a\in R$ as $I\underset{r}{\vartriangleleft} R$ so $\exists t\in R$ such that $(1-a)(1-t)=1$. The left and right inverse are the same, so $(1-a)(1-x)=1$ so x is lqr and rqr and hence it is qr.
\end{proof}

\begin{thm}
The (right) Jacobson radical is a qr ideal which contains all rqr ideals
\end{thm}
\begin{proof}
As above
\end{proof}

\begin{cor}
J(R) is left and right symmetric
\end{cor}
\begin{proof}
$J_{l}$ is a qr ideal by left and version of (9), so $J_{l}\subseteq J_{r}$, but by symmetry $J_{r}\subseteq J_{l}$ therefore $J_{r}=J_{l}=J(R)=J$.
\end{proof}

\begin{thm}[Nakayama's Lemma]
If $M$ is a finitely generated $R$-module and $J(R)M=M$ then $M=0$
\end{thm}
\begin{proof}
Suppose $M\neq 0$ and that $x_{1},\ldots ,x_{n}$ is a minimal set of generators for $M$.Then, as $J(R)=M$ $\exists r_{1} ,\ldots r_{n}\in J(R)$ such that $x_{n}=\sum_{i=1}^{n}r_{i}x_{i}$. So $(1-r_{n})x_{n}=\sum_{i=1}^{n-1}r_{i}x_{i}$. Now $r_{n}\in J(R)$ and so is qr and has a left inverse. Therefore we can write $x_{n}=\sum_{i=1}^{n-1}r_{i}x_{i}$, contradicting minimilatiy of the generating set.
\end{proof}

\begin{cor}
$R$ a ring, $M$ a fintiely generated $R$-module. Then $x_{1},\ldots x_{n}$ generate $M$ iff the image $\bar{x}_{0}\ldots \bar{x}_{n}$ generate $M/J(R)M$ as a $R/J(R)$-module.
\end{cor}
\begin{proof}
The $\Rightarrow$ proof is trivial. To prove the other direction, let $\bar{x}_{0}\ldots \bar{x}_{n}$ generate $M/J(R)M$. Take $N=Rx_{1},\ldots Rx_{n}\subseteq M$. Then $J(R)M/N=M/N$ as $N$ is preciesly all the elements that are not in the image of $J(R)M$. So by Nakayama's lemma $M/N=0$ and so $M=N$.
\end{proof}


These results will now allow us to classify $K_{0}$ for all local rings.

\begin{thm}
If $R$ is local then any projective finitely generated $R$ module $M$ is isomorphic to $R^{n}$ for some $n$. Thus $K_{0}(R)\cong \mathbb{Z}$ and is generated by $[R]$
\end{thm}
\begin{proof}
$R$ is local so $J(R)$ is maximal and $R/J(R)$ is a division ring $D$. $M$ is projective, so choose $N$ such that $M\oplus N\cong R^{k}$. Now $M/J(R)M$ and $N/J(R)N$ are modules over a division ring and so are free. The last corollary means that if they have ranks $m$ and $n$ respectively then $m+n=k$ and that we can choose a basis in $M/J(R)M$ and $N/J(R)N$ corresponding to a basis $x_{1}\ldots x_{m}\in M$ and $x_{m+1}\ldots x_{k}\in N$. These generate $R^{k}$, so if we show that they form a basis then they will be linearly independent and hence $x_{1},\ldots ,x_{m}$ will be linearly independent and will form a free basis of $M$.
\newline Let $e_{1},\ldots ,e_{k}$ be the standard basis for $R^{k}$. Then we can write $e_{i}=\sum_{j=1}^{k}a_{ij}x_{j}$ and $x_{i}=\sum_{i=1}^{k}b_{ij}e_{j}$. Then by substitution  $$e_{i}=\sum_{j=1}^{k}\sum_{l=1}^{k}b_{jl}e_{l} \Rightarrow \sum_{j=1}^{k}\sum_{l=1}^{k}(a_{ij}b_{jl}-\sigma_{il})e_{l}$$ where $\sigma_{il}$ is the Kronecker delta. As a matrix $(\sigma_{il}e_{l)=I}$ ($e_{i}$ is the standard basis), so if $A=a_(ij)$ and $B=(b_{ij})$ then we get $AB=I$.
\newline Similarly, we get the sum $\sum_{j=1}^{k}\sum_{l=1}^{k}(b_{ij}a_{jl}-\sigma_{il})x_{l}$. So, as $X_{1},\ldots ,x_{k}$ forms a basis of $R/J(R)$ then $BA=I Mod(J(R))$ and so $BA-I\in M_{n}(J(R))$. 
\newline It follows immediately from Nakayam's lemma that $J(R)M=0$ for all simple modules $M\neq 0$, as $J(R)M$ is submodule and unequal to $M$. INSERT MORE
So $M_{n}(J(R))\subseteq J(M_{n}(R))$. So $I-BA$ is qr and hence $I-(I-BA)=BA$ is invertible and so $B$ is inveritble. Right inverses are also left inverses so $BA=I$ and $x_{1},\ldots ,x_{k}$ is free basis of $R^{k}$; hence $x_{1},\ldots ,x_{m}$ generates a free module, namely $M$.
\end{proof}

\begin{cor}
Take $m\in \mathbb{Z}$ and take the prime factorisation ${p_{1}}^{q_{1}}\ldots {p_{n}}^{q_{n}}=m$. Then $K_{0}(\mathbb{Z}/(m))\cong \mathbb{Z}^{n}$.
\end{cor}
\begin{proof}
By the Chinese Remainder theorem $$\mathbb{Z}/(m)\cong \mathbb{Z}/({p_{1}}^{q_{1}})\times \ldots \times \mathbb{Z}/({p_{n}}^{q_{n}})$$ as ${p_{1}}^{q_{1}}\ldots {p_{n}}^{q_{n}}$ are pairwise coprime. $\mathbb{Z}/({p_{i}}^{q_{i}})$ is local for any $i$, so we have $K_{0}(\mathbb{Z}/({p_{i}}^{q_{i}})\cong \mathbb{Z}$ $\forall i$. So by corollary 2.2.5 we are done.
\end{proof}

\subsection{Dedekind Domains}

K-theory can be used to prove a number of algebraic number theory results. In order to to see these we will need to look at Dedekind domains, one of the main points of examination in algebraic number theory,

Throughout this section all rings $R$ will be viewed as being embedded in their field of fractions, denoted $F$. Also, all rings will be assumed to be commutative integral domains.

\begin{defn}{Fractional Ideal}
If $I\neq 0$ is an $R$-submodule of $F$, it is called a fractional ideal of $R$ if $\exists a\in R$ such that $aI\subseteq R$.
\newline An integral ideal is a fractional ideal $I$ which is already an ideal of $R$.
\newline A principal fractional ideal is a fractional ideal of the form $R\left(\dfrac{a}{b}\right)$. Note that this is a fractional ideal as $bR\left(\dfrac{a}{b}\right)\subseteq R$.
\end{defn}

\begin{prop}
The fractional ideals of aring $R$ form an abelian semigroup with identity.
\end{prop}
\begin{proof}
Associativity is inherited from the ring.
\newline $R$ is an identity element, as $1\in R$ and fractional ideals are $R$-submodules, which implies respectively that $I\subseteq RI$ and $RI\subseteq I$. Lastly, if $I,J$ , with  $a,b\in R$ auch that $aI,bJ\subseteq R$, are fractional ideals then $IJ$ is also one as $baIJ\subseteq R$.
\end{proof}


\begin{defn}[Dedekind Domain]
$R$ is a Dedekind Domain if if the fractional ideals are a group under the operation of multiplication. So, from the above, we only need that for all fractional ideals $I$ that there exists an inverse i.e. an $I^{-1}$ such that $II^{-1}=R$.
\end{defn}

\begin{prop}
If $I$ is a fractional ideal and $I^{-1}$ is such that $II^{-1}=R$, then $I^{-1}=\{a\in F|aI\subseteq R\}$.
\end{prop}
\begin{proof}
If $J=\{a\in F|aI\subseteq R\}$ then it follows that $I^{-1}\subseteq J$  by definition $J$. But then $R=II^{-1}\subseteq IJ\subseteq R$. So $II^{-1}=IJ$ and multiplying through by $I^{-1}$ gives $I^{-1}=J$.
\end{proof}

\begin{defn}
Noting that the principal fractional ideals of a Dedekind domain $R$ form a group, we can define the class group, denoted $$C(R)= \{group\ of\ fractional\ ideals\}/\{group\ of\ prinicpal\ ideals\}$$
\end{defn}

A large part of Algebraic number theory is aimed purely at calculating the class group of rings, and the follwoing proposition very explicitly shows the link between that and K-theory.

\begin{prop}
For a Dedekind domain $R$, $C(R)$ can be identified with the set of $R$-module isomorphism classes of integral fractional ideals.
\end{prop}
\begin{proof}
For any fractional ideal $I$ $\exists a\in R$ such that $aI\subseteq R$ i.e. such that $aI$ is an integral ideal, and this is clearly an isomporphism. Let $J=aI$. Then $J=aRI$ is an $R$-module isomorphism $I\rightarrow J$. 
To show the other identification, take $I\cong J$ to be $R$-modules. Let $\phi$ be the isomorphism between them and take any non-zero element, $a_{0}$, of $I$. Then $\forall a\in I$ we have $\phi(a_{0}a)=a_{0}\phi(a)=a\phi(a_{0})$, as $a,a_{0}\in R$. Therefore $\phi(a_{0})I=a_{0}J$ so $[I]=[J]$ in $C(R)$.
\end{proof}

\begin{thm}
Every fractional ideal of a Dedekind domain is finitely generated and projective. This also means that $R$ is Noetherian
\end{thm}
\begin{proof}
Let $I$ be a fractional ideal. $II^{-1}=R$, therefore $\exists x_{1},\ldots ,x_{n}\in I$ and $y_{1},\ldots ,y_{n}\in I^{-1}$ such that $\sum_{i=1}^{n}x_{i}y_{i}=1$. So $\forall b\in I$ we have $b=\sum_{i=1}^{n}(bx_{i})y_{i}=1$ with $bx_{i}\in I^{-1}I=R$ and therefore $y_{1}\ldots ,y_{n}$ generate $I$. Thus every ideal of $R$ is finitely generated and hence $R$ is Noetherian. However, the sum also gives us that if we take $\psi :R^{n}\rightarrow I$, where $\psi(r_{1},\ldots ,r_{n})=\sum_{i=1}^{n}a_{i}y_{i}$, then we have a right inverse, where $\psi ^{-1}(b)=(bx_{1},\ldots bx_{n})$. Therefore it splits and so $I$ is projective.
\end{proof}
\textbf{NEED TO ADD IN PROOF THAT SPLIT IMPLIES PROJECTIVE AT START}

\begin{cor}
If $R$ is a Dedekind domain then every finitely generated projective module over $R$ is isomorphic to a direct sum of ideals of $R$. It follows that $K_{0}(R)$ is generated by the isomorphism classes of ideals.
\end{cor}
\begin{proof}
Proof is as in Theorem 2.3.1.
\end{proof}

\begin{thm}
Let $R$ be a Dedekind domain. Then the following are true:
\begin{enumerate}
		\item Every prime integral ideal is maximal
		\item Every proper integral ideal can be factored in to prime ideals uniquley (up to ordering)
		\item The group of fractional ideals is the free  abelian group (under multipliction) on the non-zero prime ideals.
	\end{enumerate}
\end{thm}
\begin{proof}
\begin{enumerate}
 \item Take $I$ to be a non-zero prime integral ideal and suppose it is not maximal.. So there exists an inegral ideal, say $J$, such that $I\subsetneq J\subsetneq R$. Let $K=J^{-1}I$. Then $I\subsetneq I\Rightarrow K\subsetneq JJ^{-1}=R$. Also, $JK\subsetneq I$, $I$ is prime and $J\nsubseteq I$ so $K\subseteq I$. But this means that $I=JK\subseteq JI\subsetneq RI=I$. This is a contradiction, so $I$ is maximal.

\item Existence: Let $C$ be the set of all proper integral ideals  that are not the product of prme ideals. If it is empty we are done. If not, as $R$ is Noetherian by 2.5.7 then every ascending chain of ideals has a maximal element and so applying Zorn's lemma gives that there exists a maximal element $I$. $I$ cannot be maximal, else it would be an integral prime ideal. So choose $I_{1}$ such that $I\subsetneq I_{1}\subsetneq R$. Set $I_{2}=II_{1}$. Notice that $I_{2}\subsetneq R$ as $I\subsetneq I_{1}\Rightarrow I_{1}^{-1}I\subsetneq I_{1}^{-1}I_{1}=R$. Also, $I\subsetneq I_{2}$ as $R\subsetneq I_{1}^{-1}\Rightarrow I=IR\subsetneq I_{1}^{-1}$. So $I_{1},I_{2}\supsetneq I\Rightarrow I_{1},I_{2}\notin C$. Therefore they factorise into prime ideals. But $I=I_{1}I_{2}$. Contradiction.

 Uniqueness:  Suppose $I=P_{1}\ldots P_{n}=Q_{1}\ldots Q_{m}$ where $P_{i},Q_{j}$ are prime for all $i,j$ and WLOG assume $n\leq m$. So as $P_{1}\supseteq P_{1}\ldots P_{n}=Q_{1}\ldots Q_{m}$ then at least one $Q_{j}\subseteq P_{1}$. Reorder so that $j=1$ and then as $Q_{1}$ is prime and hence maximal, and $Q_{1}\subseteq P_{1}\subsetneq R$ then we have that $P_{1}=Q_{1}$. Similarly, for any $P_{i}\ \exists Q_{j}$ such that $P_{i}=Q_{j}$. Re-order so that $\forall i\leq n\  P_{i}=Q_{i}$. Then $P_{1}\ldots P_{n}=P_{1}\ldots P_{n}Q_{n+1}\ldots Q_{m}$, so $Q_{n+1}\ldots Q_{m}=R$, so as $Q_{i}$ are all integral ideals then $Q_{i}=r$ for $i>n$ and we are done.

\item There is the natural map from the free abelian group of the prime ideals to the group of the field of fractions under multiplication. Part 2) of the proof shows that is is surjective. To show injectivity we need to show the kernel is trivial, i.e., just $R$. So assume this is not the case. Then $\exists P_{1}^{n_{1}}\ldots P_{m}^{n_{m}}$ such that $P_{i}$ are prime ideals and for some $n_{i}$ not all zero $P_{1}^{n_{1}}\ldots P_{m}^{n_{m}}=R$. Then some $n_{i}<0$, otherwise $P_{1}^{n_{1}}\ldots P_{m}^{n_{m}}\subsetneq R$. So if $n_{k}<0$ then muliply by $P_{k}^{|n_{k}|}$ and we get that the ideal $P_{k}^{|n_{k}|}$ has two distinct factorisations.
\end{enumerate}
\end{proof}

\begin{lem}
If $R$ is commutative and $I_{1},\ I_{2}$ are such that $I_{1},I_{2}\triangleleft R$ and $I_{1}+I_{2}=R$, then $I_{1}I_{2}=I_{1}\cap I_{2}$.
\end{lem}
\begin{proof}
$I_{1}I_{2}\subseteq I_{1}\cap I_{2}$ trivially. For the other inclusion, $I_{1}+I_{2}=R$ so we can choose $a_{1}\in I_{1}$ and $a_{2}\in I_{2}$ such that $a_{1}+a_{2}=1$. So $\forall x\in I_{1}\cap I_{2}$ we have $x=a_{1}x+xa_{2}\in I_{1}I_{2}$
\end{proof}

\begin{lem}
Let $R$ be a Dedekind domain, $I$ any fractional ideal, $J$ any integral ideal. Then $\exists a\in I$ such that $I^{-1}a+J=R$
\end{lem}
Let $P_{1}\ldots P_{n}$ be the prime facorisation of $J$. For each $i$ chose $a_{i}\in IP_{1}\ldots \hat{P_{i}}\ldots P_{n}$ such that $a_{i}\notin IP_{1}\ldots P_{n}$ and take $a=\sum_{i=1}^{n}a_{i}$. Now $$a_{i}I^{-1}\subseteq P_{1}\ldots \hat{P_{i}}\ldots P_{n} \subseteq P_{j}$$ for all $i\neq j$. If $i=j$ then  we would have $a_{i}I^{-1}\subseteq \underset{j}{\bigcap} P_{j}=P_{1}\ldots P_{n}$ by applying 2.5.10, and so $a_{i}\in IP_{1}\ldots P_{n}$. It follows that $aI^{-1}\subsetneq P_{j}$ for all $j$. $I^{-1}$ is an integral ideal, so $aI^{-1}+J$ is too and by the preivous statement no $P_{j}$ divides it. However, the choice of means that no other prime ideal can divide it, hence it cannot be a proper ideal and must be $R$.

\begin{cor}
Any fractional ideal of a Dedekind domain can be generated by at most two elements of $R$.
\end{cor}
\begin{proof}
Let $I$ be a fractional ideal. Choose non-zero $b$ in $I$. Let $J=bI^{-1}$, an integralideal. Then, by the previous lemma, there exists $a\in I$ such that $aI^{-1}+bI^{-1}=R$, so $I=Ra+rB$
\end{proof}

\begin{lem}
If $R$ is a Dedkind domain and $I_{1},I_{2}$ are fractional ideals then $I_{1}\oplus I_{2}\cong R\oplus I_{1}I_{2}$ as $R$-modules.
\end{lem}
\begin{proof}
Choose non-zero $a_{1}\in I_{1}$ and take $J$ in 2.5.10 to be $a_{1}I^{-1}$. Thenthere exists $a_{2}\in I_{2}$ such that $I_{2}^{-1}a_{2}+a_{1}I_{1}^{-1}=R$.  So take $b_{1}\in I_{1}^{-1}$ and $b_{2}\in I_{2}^{-1}$ such that $a_{1}b_{1}+a_{2}b_{2}=1$. Then
$$\left( \begin{array} {cc}
b_{1} & -a_{2} \\
b_{2} & a_{1} \end{array} \right)
\left( \begin{array} {cc}
a_{1} & a_{2} \\
b_{2} & b_{1} \end{array} \right)=
\left( \begin{array} {cc}
1 & 0 \\
0 & 1 \end{array} \right)$$
So $\left( \begin{array} {cc}
b_{1} & -a_{2} \\
b_{2} & a_{1} \end{array} \right)$ is invertible, hence defines an isomorphism from $I_{1}\oplus I_{2}\rightarrow R\oplus I_{1}I_{2}$, where $(x_{1},x_{2})\mapsto (x_{1},x_{2})\left( \begin{array} {cc}
b_{1} & -a_{2} \\
b_{2} & a_{1} \end{array} \right)$
\end{proof}

\begin{thm}
Let $R$ be Dedekind. Any projective $R$-module of rank $k$ is isomorphic to $R^{k-1}\oplus I$ for some ideal $I$, where the isomorphism class of $I$ is uniquely determined. Moreover, if $P$ and $Q$ are projective modules of the same rank, $P\cong R^{k-1}\oplus I$ and $Q\cong R^{k-1}\oplus J$, $I,J\triangleleft R$, then the map $[P]-[Q]\mapsto IJ^{-1}$ is an isomorphism from $\tilde{K}_{0}(R)$ to $C(R)$. \newline Furthermore, $[R^{k-1}\oplus I]\mapsto (k,[I])$ gives a homorphism $K_{0}(R)\rightarrow \mathbb{Z}\oplus C(R)$.\newline As a commutative ring $K_{0}(R)\cong\{(k,[I]):k\in \mathbb{Z}; [I]\in C(R)\}$, with operations in the latter group
\begin{enumerate}
\item $(k,[I])+(k',[I'])=(k+k',[I]+[I'])$
\item $(k,[I])\times (k',[I'])=(k\times k',[I]^{k}\times [I']^{k'})$
\item rank:$(k,[I])\mapsto k\in \mathbb{Z}$
\end{enumerate}
\end{thm}
\begin{proof}
From 2.5.8, every finitely generated projective module $P$ is isomoprhic to a direct sum of ideals, $I_{1}\oplus\ldots\oplus I_{k}$. Also, by \textbf{1.3.12 in book, need to put in projet or assume} $P$ has well defined rank. If $I$ is an ideal, then $Rank(I)=dim_{F}(F\otimes_{R}I)=dim_{F}(F)=1$, so the rank of $P$ is the number of ideals in the sum (i.e.$k$). The previous lemma allows us to write this sum as $R^{k-1}\oplus I$ where $I=\Pi_{i=1}^{k}I_{i}$. So it remains to show that the isomorphism class is uniquely determined. In other words that $R^{k-1}\oplus I_{1}\cong R^{k-1}\oplus I_{2}$ implies that $I_{1}\cong I_{2}$. By 2.5.6, this is equivalent to $[I_{1}]=[I_{2}]$.
\\ So suppose there exists an isomorphism $\alpha:R^{k-1}\oplus I_{1}\rightarrow R^{k-1}\oplus I_{2}$. As in the proof of 2.5.6, an $R$-module map from one ideal to another is simply multilpication by some element of $F$,; so $\alpha, \alpha^{-1}$ are equivalent to multiplication by $k\times k$ matrices. If $X$ is the diagonal matrix with diagonal $(1,\ldots 1,x)$. where $x\in I_{1}$ (call $x$ the corrseponding map) then $X$ gives a map from $R^{k}\rightarrow R^{k-1}\oplus I_{1}$, and so $X\alpha$ gives a map $R^{k}\rightarrow R^{k-1}\oplus I_{2}$.\newline Now the matrix $XA$ has as its rows the image of the standard basis of $R^{k}$ under the map $\alpha x:R^{k}\rightarrow (R^{k-1}\oplus I_{2})$, so all of the bottom column must be in $I_{2}$. Therefore if, when we calcuate $Det(XA)$ we start by expanding the bottom row, and see that $Det(XA)\in I_{2}$. As $DetX=x$ this means that $xDetA\in I_{2}$, so $DetA$ maps every element of $I_{1}$ to $I_{2}$.
\\ Similarly, $DetA^{-1}=(DetA)^{-1}$ maps every element of $I_{2}$ to $I_{1}$ and is clearly an inverse map, so we have the desired isomorphism.
\end{proof}

\begin{defn}
If $S$ is a subring of $R$ then it is integrally closed if in $R$ if all roots of monic polynomials $f(x)\in S[X]$ that are in $R$ are also in $S$.
\end{defn}

 For example, $\mathbb{Z}[X]$ is integrally closed over $\mathbb{Q}$.

\begin{lem}
Let $R$ be a Noetherian integral domain which is integrally closed in its field of fractions $F$ and let $I$ be a fractional ideal of $R$. Then $R=\{ s\in F|sI\subseteq I\}$
\end{lem}
\begin{proof}
$R$ is Noetherian and hence finitely generated, so $I$ is also finitely generated. Let $S=\{s\in F|sI\subseteq I\}$. $I$ is an ideal of $R$, so $R\subseteq S$. By showing that that all elements of $S$ are integral over $R$ we will show that $S\subseteq R$ as $R$ is integrally closed. Let $\{a_{j}\}$ be the generators of $I$. Then, as $sI\subseteq I\subseteq R$ we have that $sa_{j}=\Sigma b_{jk}a_{k}$. Let $B=(b_{jk})$; then $s$ is an eigenvalue, as $\sum s(a_{1},\ldots a_{m})= B(a_{1},\ldots a_{m})$. Therefore $s$ is a root of the characterisitc equation, which is monic with co-efficients in $R$ and hence is in $R$ as it is interally closed. So $S=R$
\end{proof}
\begin{lem}
Any non-zero proper ideal $I$ of a Noetherian commutative ring $R$ conatins a product of prime ideals.
\end{lem}
\begin{proof}
Let $C=\{I\triangleleft R|I \ contains \ no \ product \ of \ prime \ ideals\}$. We want to show that $C=\Phi$.
\\ Suppose $C$ is non-empty. $R$ is Noetherian, hence $C$ has a maximal element; call it $I$. $I$ is not prime, otherwise $I$ would trivially contain the prime ideal $I$. So, there exsists $a,b\in R$ with $a,b\notin I$ but $ab\in I$. Clearly $I\subsetneq I+aR$ and $I\subsetneq I+bR$. If $I+aR=R$ then $(I+aR)(I+bR)=R(I+bR)\supsetneq I$ and at the same time we would have $(I+aR)(I+bR)\subseteq I+abR\subseteq I$, which is a contradiction.
\\ Therefore $I+aR$ is properly conatined in $R$, as is $I+bR$ by the same argument. As $I$ was maximal there are prime factorisations conatined in both $I+aR$ and $I+bR$; call them $P_{1}\ldots P_{n}$ and $Q_{1}\ldots Q_{m}$ respectively. But this gives $I\subseteq (I+aR)(I+bR)\subseteq P_{1}\ldots P_{n}Q_{1}\ldots Q_{m}$, which contradicts $I\in C$.
\end{proof}

\begin{lem}
Let $R$ be Noetherian and integral and have all prime ideals maximal. Thanfor any non-zero proper ideal $I$ of $R$ there is some $c\in F$ such that $c\notin R$ and $cI\subseteq R$.
\end{lem}
\begin{proof}
Let $a\neq0$ in $I$. $Ra$ is an ideal of $R$ and hence contains a product of non-zero prime ideals by the previous lemma. Let $P_{1}\ldots P_{m}$ be such a product in $I$ with minimal $m$. Let $P$ be a maximal ideal such that $P\supseteq I$. Then we have $P_{1}\ldots P_{m}\subseteq Ra\subseteq I\subseteq P$ therefore, as $P$ is prime some $P_{i}\subseteq P$, and prime implies maximal, so $P=P_{1}$. Now take cases. Firstly, $m=1$ we have $I=Ra=P$ is maximal, $a^{-1}\notin R$ (if $a^{-1}\in R$ then $I=R$) and obviously $a^{-1}I\subseteq R$.
\newline If $m\neq 1$ then $m\geq 2$. Then $Ra\nsupseteq P_{2}\ldots P_{m}$ as $m$ is minimal. So we can chose $b\notin Ra$, and let $c=b\dfrac{b}{a}\in F$. Then $c\notin R$, else $b\in R$, but we also have that $cI\subseteq cP_{1}=a^{-1}bP_{1}\subseteq a^{-1}P_{1}\ldots P_{m}\subseteq a^{-1}Ra=R$.
\end{proof}

\begin{thm}
Let $R$ be a commutative integral domain. Then $R$ is Dedekind iff it satisfies the following:
\begin{enumerate}
 \item Every non-zero prime ideal is maximal
 \item $R$ is integrally closed in its fields of fractions
 \item $R$ is Noetherian
\end{enumerate}
\end{thm}
\begin{proof}
Theorem 2.5.7 shows that 3) always holds for $R$ Dedekind, and 1) holds by part one of theorem 2.5.9. So it remains to show that $R$ being Dedekind implies $R$ is integrally closed.
\\ Take $a\in F\setminus \{0\}$, $a$ integral over$R$. So there is a monic polynomial in $R[X]$ which solves for $a$; call its co-efficients $a_{0},\ldots a_{n-1}$. Let $M=R+Ra+Ra^{2}+\ldots Ra^{n-1}$. This is an $R$-submodule of $F$, as $a^{n}=-a_{0}-\ldots -a_{n-1}a^{n-1}\in R+Ra+Ra^{2}+\ldots Ra^{n-1}$ it is stable under multiplication by $a$as a module. As $a\in F$ we can write it as $\dfrac{p}{q}$ with $p,q\in R$ and $q\neq 0$. Then $q^{n-1}M\subseteq R$ and so $M$ is a fractional ideal. As $aM\subseteq M$ we get that $aR=aMM^{-1}\subseteq MM^{-1}=R$ and so $a\in R$.
\newline To show the other implication, assume that $R$ satisfies the conditions and let $I$ be a fractional ideal of $R$. Define $J=\{a\in F|aI\subseteq R\}$. In a Dedekind domain $J=I^{-1}$ by definition , so want to show that $IJ=R$. $IJ$ is an integral ideal by definition of $J$. Now define $K=\{a\in F|aIJ\subseteq R\}$. Then $R\supseteq K(IJ)=(KJ)I$ by commutativity and the definition of $K$. Also, by definition of $J$ being all the elements $a$ such that $aI\subseteq R$, we have that $KJ\subseteq J$. So by 2.5.16 we have that $K\subseteq R$. But if $IJ\subsetneq R$ then the previous lemma implies that $K\supsetneq R$, contradicting 2.5.16. Therfore $J=I^{-1}$.
\end{proof}

\begin{defn}
A number field $F$ is a finite field extenstion of $\mathbb{Q}$; i.e the field rationals with non-rational number(s) that solve(s) for some $p(x)\in \mathbb{Z}[x]$.
\end{defn}
 For example, $x^{2}-2\in \mathbb{Z}[x]$ and $\sqrt{2}\notin \mathbb{Q}$ is a solution, so we can take the number field $F=\mathbb{Q}[\sqrt{2}]=\{a+b\sqrt{2}|a,b\in \mathbb{Q}\}$.

\begin{defn}
The algebraic integers of a number field $F$ are the elements of $F$ that solve for some monic polynomial in $\mathbb{Z}[x]$.
\end{defn}

\begin{thm}
For any number field $F$ the algebraic integers , $R$, of that field are a Dedekind Domain.
\end{thm}
\begin{proof}
It suffices to check the three conditions in the previous theorem. Showing that $R$ is integrally closed in $F$ is clear. \textbf{need to do}
\\ Part 1) can be proven with the following method. Let $P\neq 0$ be a prime ideal in $R$. Then $P\cap \mathbb{Z}$ must be prime in $\mathbb{Z}$ as it is a PID. Suppose $P\cap \mathbb{Z}=0$. Then take non-zero $b\in P$. Let $b=b_{1},b_{2},\ldots b_{n}$ be the conjugates of $b$. Then $\prod_{i=1}^{n}(-1\times b_{i})$ is the constant term of the minimal polynomial for $b$, and as $b\in R$ the minimal polynomial is in $\mathbb{Z}[x]$, so $\prod_{i=1}^{n}(-1\times b_{i})\in \mathbb{Z}$. Let $c=\prod_{i=2}^{n}(-1\times b_{i})$. Obviously each $b_{i}$ is an algebraic integer, solving the same polynomial as $b$, so as $R$ is a ring, there product is an algebraic integer and $c\in R$.So we conclude that $0\neq bc\in \mathbb{Z} \cap P\subseteq \mathbb{Z}\cap P$ and the intersection is a non-zero prime ideal in $\mathbb{Z}$. As $F$ is a finte exension, $R/P$ is contained in a finite extension of $\mathbb{Z}/(P\cap \mathbb{Z}=\mathbb{Z}/(p)\cong \mathbb{F}_{p}$. An extension of this is a finte integral domain, which is a field which implies $P$ is maximal.
\newline Finally, it remains to show that $R$ is Noetherian. If the order of the extension from $\mathbb{Q}$ to $F$ is $n$, then every element of $R$ has a monic degree $n$ polynomial that solves for it. The $Tr(x):=\sum_{i=1}^{n}\beta_{i}$ where $\beta_{i}$ are the roots of these equations. Then this is in $\mathbb{Z}$ as it is the co-efficient of $x^{n-1}$. Then the map $x\mapsto (Tr(xa_{1}),\ldots ,Tr(xa_{n}))$. where $\{a_{i}\}_{i=1}^{n}$ is a basis of $F$ over $\mathbb{Q}$ in $R$ and maps $R$ to $\mathbb{Z}^{n}$. So $R$ is a finitely generated $\mathbb{Z}$ module, so an ascending chain of ideals of $R$ corresponds to an ascending chain of ofsubmodules in $\mathbb{Z}^{n}$ and so terminates.
\end{proof}

\begin{thm}
Let $R$ be the ring of algebraic integers of a number field $F$. Then the class group $\tilde{K}_{0}(R)$ is finite.
\end{thm}
\begin{proof}
To start, define a norm on ideals of $R$ as follows. Let $I$ be an ideal and let $P_{1}^{n_{1}},\ldots ,P_{m}^{n_{m}}$ be it's prime factorisation. Then by Chinese remainder theorem $R/I\cong R/P_{1}^{n_{1}}\times P_{m}^{n_{m}}$. Now, $R/P_{i}$ is a field as $R$ is Dedekind and so prime implies maximal. Also noticing that there is a composition series for $R/P_{i}^{n_{i}}$, namely $\{0\}\subseteq R/P_{i}\subseteq R/P_{i}^{2} \ldots \subseteq R/P_{i} \subseteq  R$ and at each of the $n_{i}$ steps the composition factor is $R/P_{i}$, so $R/P_{i}^{n_{i}}$ has finite order of $|R/P_{j}|^{n_{i}}$, so $R/I$ is finite. This now allows us to define the norm as: $$||I||=|R/I|=\prod_{I=1}^{m}|R/P_{i}|^{n_{i}}=\prod_{i=1}^{m}||P_{i}||^{n_{i}}$$.
Notice that this is multiplicative (i.e. $||I_{1}I_{2}||=||I_{1}||\times ||I_{2}||$ as the prime ideal will just pultiply. Also, note that this can be viewed as the determinant of the map from $R\rightarrow R/I\subseteq R^{m}\cong_{\mathbb{Z}} \mathbb{Z}^{m}$.

Now, if $I$ is prinicpal, generated by $(a)$ say. Then $R$ has finite generators, say $n$ where $n=[R:\mathbb{Z}]$, and these are free, so is isomorphic to $\mathbb{Z}^{n}$, so $a:\mathbb{Z}^{n} \rightarrow \mathbb{Z}^{n}$. Therefore we can represent $a$ by a matrix $A$, and $Det(A)=Norm(a)=N(a)$, and as the image of $A$ is obviously $Ra$ then $[Ra:R]=N(a)$. ALso, clearly $||(a)||=|R/(a)|=[Ra:R]$.

From the last proof we saw that if $P$ is a prime idel then $P\cap \mathbb{Z}=(p)$ for some prime $p$, and as $R/P$ is a finite extension of $\mathbb{Z}/(p)$, the degree of which is clearly less than $n=[F:\mathbb{Q}]$, then $||P||=p^{i}\ 1 \leq i \leq n$. So if $||P||\leq c$, some $c>0$ and $p\leq c$. However the only ideals that contain $p$ can be those that form the ideal factorisation of $Rp$, of which there are a finite number, so for any $c>0$ there finite ideals $I$ such that $||I||\leq c$. So if there is some $C>0$ such that all elements of $C(R)$ have a representative with $||I||\leq C$ then we are done.

Let $x_{1},\ldots x_{n}$ be a basis for $R$ as a $\mathbb{Z}$. Let $$M=max\{|y_{i}|:B_{i}\ are\ the\ conjugates\ of\ some\ x_{i}\}$$ and define $C=n^{n}M^{n}$. Any element $I$ of $C(R)$ can be represented by it's inverse $J=I^{-1}$, where $I$ is integral. So we want to find an ideal in the equivalence class of $I$ and $K$, with $||K||\leq C$. Define $S$ to be the set $S=\{a_{1}x_{1}+\ldots +a_{n}x_{n}|a_{i}\in \mathbb{N}, a_{i}\leq \lceil||I||^{\frac{1}{n}}\rceil \}$. Then $|S|=(\lceil||I||^{\frac{1}{n}}\rceil +1)^{n}>||I||=|R/I|$. Therefore there is no injectie map from $S$ to $R/I$. Let $\kappa$ and $\iota$ be two elements mapped to the same point. Then let $\mu =\kappa - \iota \in J$ and let $K=(\mu)J^{-1}=(mu)I\cong I$.So $(\mu )=JK$, therefore $||I||||K||=||(\mu )||=|N(\mu )|$.

Also, as $\kappa,\iota \in S$ then we can write $\mu = b_{1}x_{1}+\ldots +b_{n}x_{n}$ with $|a_{j}|\leq \lceil ||J||^{\frac{1}{n}}\rceil$. Combining these gives $$||I||||J||=|N(\mu )| = \prod_{\sigma}|b_{1}\sigma(x_{1})+\ldots +b_{n}\sigma (x_{n})|$$ $$\leq \prod_{\sigma}|b_{1}||\sigma(x_{1})|+\ldots +|b_{n}||\sigma(x_{n})|$$ $$\leq \pod_{\sigma}(n\lceil ||I||^{\frac{1}{n}}\rceil M)\leq n^{n}||I||M^{n}=C||I||$$.
\end{proof}


\subsection{Relative $K_{0}$ and Excision}

In this section the aim is to define relative $K_{0}(R)$ for a ring $R$ and use this in the excision theorem. This will enable $K_{0}(R)$ to be calculated in many cases much more easily, or even at all. Throughout this section all ideals will be assumed to be two sided.

\begin{defn}
Let $R$ be a ring and $I\triangleleft R$ an ideal of $R$. The double of $R$ along $I$ is $D(R,I)=\{(x,y)|x,y\in R,x-y\in I\}\subseteq R\times R$.
\end{defn}

Let $p_{1}:D(R,I)\rightarrow R$ be the map $(x,y)\mapsto x$. Then the exact sequence $$0\rightarrow I\rightarrow D(R,I)\rightarrow R\rightarrow 0$$ is split, so $I\cong ker(p_{1})$.

Given this definition and noticing the above, we are now in a position to define the relative $K_{0}$ group.

\begin{defn}
If $R$ is a ring and $I$ is an ideal of this then the relative $K_{0}$ group of the pair $(R,I)$ is $$K_{0}(R,I)=ker((p_{1})_{*}:K_{0}(D(R,I))\rightarrow K_{0}(R))$$.
\end{defn}

Now we require one lemma which will be useful throughout the rest of the essay before moving building up to the proof of the excision theorem.

\begin{lem}
Let $R$ be a ring and $I$ an ideal of $R$. Then for any $A\in GL(n,R/I)$, then the $2n\times2n matrix \left(\begin{array}{cc} A & 0 \\ 0 & A^{-1} \end{array}\right)$ lifts to a matrix in $GL(2n,R)$.
\end{lem}
\begin{proof}
There is an identity $$\left(\begin{array}{cc} A & 0 \\ A^{-1} & 0 \end{array}\right)=
\left(\begin{array}{cc} 1 & A \\ 0 & 1 \end{array}\right)
\left(\begin{array}{cc} 1 & 0 \\ -A^{-1} & 1 \end{array}\right)
\left(\begin{array}{cc} 1 & A \\ 0 & 1 \end{array}\right)
\left(\begin{array}{cc} 0 & -1 \\ 1 & 0 \end{array}\right)$$
So if each matrix on the right lifts, so does the matrix on the left.

Firstly, $\left(\begin{array}{cc} 0 & -1 \\ 1 & 0 \end{array}\right)$ lifts to itself.

Now suppose $B,C\in M_{n}(R)$ such that they quotient to $A$ aqnd $A^{-1}$, then regardless of $B$ and $C$ the matrices $$\left(\begin{array}{cc} 1 & B \\ 0 & 1 \end{array}\right)\ and\ \left(\begin{array}{cc} 1 & 0 \\ C & 1 \end{array}\right)$$ have det=1 and so are invetible such that they quotient to $\left(\begin{array}{cc} 1 & A \\ 0 & 1 \end{array}\right)$ $\left(\begin{array}{cc} 1 & 0 \\  & 1 \end{array}\right)$, so we are done.
\end{proof}

\begin{thm}
Let $R$ be a ring, $I\triangleleft R$. Then there is a natural short exact sequence $$K_{0}(R,I)\rightarrow K_{0}(R) \rightarrow K_{0}(R/I)$$ where $q_{*}:K_{0}(R)\rightarrow K_{0}(R/I)$ is induced by the quotient map and $p_{2*}:K_{0}(R,I)\rightarrow K_{0}(R)$ is induced by the projection of the second co-ordinate of $D(R,I)$.
\end{thm}
\begin{proof}
Let $a=(a_{1},a_{2}),b=(b_{1},b_{2})\in Idem(D(R,I)$ and take $[a]-[b]\in K_{0}(R,I)$. Obviously $K_{0}(R,I)\subseteq K_{0}(R\times R)\cong K_{0}(R)\times K_{0}(R)$, and in $K_{0}(R\times R)$ it is naturally mapped to $([a_{1}]-[b_{1}],[a_{2}]-[b_{2}])$. By definition of $K_{0}(\circ R,I)$ being the kernel of $(P_{1})_{*}$, we have that $q_{*}\circ (P_{1})_{*}=0$. Also, as $a,b\in D(R,I)$ then after quotienting by $I$ we get that $bar{a}_{1}=\bar{a}_{2}$ and $\bar{b}_{1}=bar{b}_{2}$, $q_{*}\circ (P_{2})_{*}=bar{a}_{2}-\bar{b}_{2}=bar{a}_{1}-\bar{b}_{1}=q_{*}\circ (P_{1})_{*}=0$. So image is contained in the kernel.

To show the other inclusion, take $a,b\in Idem(R)$ such that $q_{*}([a]-[b])=[\bar{a}] -[\bar{b}]=0$. i.e. $[a]-[b]\in ker(q_{*})$. As any element of $K_{0}(R)$ can be written in this form for any ring $R$, this covers all of the kernel of $q_{*}$ \textbf{need to write theorem in}. So $[\bar{a}]$ is stably isomorphic to $[\bar{b}]$ and there exists $k$ such that $I_{k}$ (which is equivalent to $[R^{k}]$ as a module) satisfies $q(a\oplus I_{k})=\bar{a}\oplus \bar{I}_{k}\sim \bar{b}\oplus \bar{I}_{k}=q(b\oplus I_{k})$, where conjugation is taken over $GL(R/I)$. Let $x=a\oplus I_{k}$ and $y=b\oplus I_{k}$.

Then there is some matrix $\bar{z}\in GL(R/I)$ such that $\bar{x}=\bar{z}\bar{y}(\bar{z})^{-1}$. Now the previous lemma says that $\bar{z}\oplus(\bar{z})^{-1}$ does lift to a matrix $w\in GL(R)$. Noting that this clearly satisfies $\bar{x}\oplus 0=(\bar{z}\oplus(\bar{z})^{-1})(\bar{y}\oplus 0)((\bar{z})^{-1}\oplus\bar{z})$. So by replacing $x$ by $w(x\oplus)w^{-1}$ in $GL(R)$ we have a matrix which is similar, and so in the same conjugacy class, but also has $\bar{x}=\bar{y}$. So now $(x,y)\in Idem(R,I)$. Then $[(a,a)]-[(a,b)]\in K_{0}(D(R,I))$ is clearly in $ker((P_{1})_{*})$ and to maps $[a]-[b]$ under $(p_{2})_{*}$. So it maps surjectivly to the kernel and the sequence is exact.

The naturality follows from the fact that $K_{0}$ is a functor.
\end{proof}

To continue to the proof of the excision theorem it is necessary to define how one can add a unit to a an ideal. As every ideal is a ring, this can be done for the more genral case of a ring without unit.

\begin{defn}
Let $I$ be a ring without a unit. Then we define $I_{+}=I\oplus \mathbb{Z}$, where multiplication is defined to be $(x,n)\times (y,m)=xy+ny+mx,mn)$ and addition is defined in the obvious way. The unit element is $(0,1)$.

If there exists a homomorphism $h:I\rightarrow J$ then there is only one extension to a homomorphism $h`:I_{+}\rightarrow J_{+}$.
\end{defn}

\begin{defn}
Let $I$ be a ring which does not necessarily have a unit. There is a split exact sequence $$0\rightarrow I\rightarrow I_{+}\rightarrow \mathbb{Z}\rightarrow 0$$
Then define $$K_{0}(I)=ker(\rho_{*}:K_{0}(I_{+})\rightarrow K_{0}(\mathbb{Z})\cong\mathbb{Z})$$
\end{defn}

For the purpose of this definition it is necessary to show that what happens when $I$ already has a unit under this definition is the what we would expect from the previous definiton of $K_{0}(R)$. So if $1_{I}\in I$ is a unit in $I$, then there is an isomoprhism $h:I_{+}\rightarrow I\times \mathbb{Z}$, where $(x,n)\mapsto (x+n1_{I},n)$. This shows how the sequence is split exact in the case where $I$ has an identity.Also, as $K_{0}(I_{+})\cong K_{0}(I)\oplus K_{0}(\mathbb{Z})$ then we get, as desired $K_{0}(I)=ker(\rho_{*})$.

\begin{thm}
Let $I$ be a two sided ideal of a ring $R$. Then $K_{0}(R,I)\cong K_{0}(I)$.
\end{thm}
\begin{proof}
Let $\alpha:I_{+}\rightarrow D(R,I)$ be the homorphism $(x,n)\mapsto (n\times1,n\times1+x)$ where $x\in I,\ n\in\mathbb{Z}$. Also note that, with above homoorphism, the following commutes:
\newline \textbf{insert commutaive diagram}

So $\alpha_{*}:K_{0}(I_{+})\rightarrow K_{0}(D(R,I))$ maps $ker(\rho_{*})=K_{0}(I)$ to $ker(p_{1})_{*}=K_{0}(R,I)$, as $\alpha$ just embeds $\mathbb{Z}$ in to the first co-ordinate of $D(R,I)$.

So now we need to show surejectivity and injectivity of $\alpha _{*}$. Take a class $[a]-[b]\in K_{0}(R,I)$, with $a=(a_{1},a_{2}),\ b=(b_{1},b_{2})\in Idem(D(R,I)$ and with $[a_{1}]=[b_{1}]$. So there exists $k$ such that taking $a\oplus I_{k}$ and $b\oplus I_{k}$ means that $a_{1}\sim b_{1}$ under multiplication in $GL(R)$. In other words $\exists g\in GL(R)$ such that $a_{1}=gb_{1}g^{-1}$. So replace $a$ by $(ga_{1}g^{-1},ga_{2}g^{-1})$. This has the same class as $a_{1},a_{2})$, so we may assume that $a_{1}=b_{1}$. Now, if $e$ is a matrix, say $n\times n$, then we can replace it and $b$ by $a\oplus (I_{n}-a_{1},I_{n}-a)$ and $b\oplus I_{n}-b_{1},I_{n}-b_{1}$. As $a$ is invertible and idemptoent then $I_{n}-a$ is not invertible and so similar to the zero matrix, so $e_{1}\oplus I_{n}-e_{1}\sim I_{n}\oplus 0$. So we can conjugate to get $a=(I_{n}\oplus 0_{n},a_{2})$, $b=(I_{n}\oplus 0_{n}, b_{2})$. As $a,b \in D(R,I)$ then $a_{2}-(I_{n}\oplus 0_{n}),\ b_{2}-(I_{n}\oplus 0_{n})$. So $[a]-[b]\in K_{0}(I)$.

Lastly we need to show that $\alpha_{*}$ is injective. Take $[a]-[b]\in K_{0}(I)$ with $a,b\in idem(I_{+})$ with $rank \rho(a)=rank\rho(b)$. Again, if $b$ is an $n\times n$ matrix we can again take a direct product with $I_{n}-b$ and conjugate to assume that $b=I_{n}$, so $rank\rho(a)=n$, and $\exists g\in GL(\mathbb{Z})$ such that $g\rho(a) g^{-1}=I_{n}$.. So we could take $g$ to be in $GL(I_{+})$ using the exact sequence in definition 2.6.6 and the fact that it extends to $GL$ of each ring. So $a$ can be replaced by $gag^{-1}$ so can assume  $\rho(a)=I_{n}$. So this means that $\alpha([a]-[I_{n}])=0$ if and only if $[(I_{n},a)]=[(I_{n}.I_{n})]\in K_{0}(D(R,I))$ So these are stably isomorphic  and so there exists $k\geq n$ and $f_{1},f_{2})\in GL(D(R,I))$ with $f_{2}af_{2}^{-1}=I_{k}$. So $(I_{k},f_{1}^{-1}f_{2})\in GL(D(R,I))$ and$(f_{1}^{-1})a(f_{1}^{-1}f_{2})^{-1}=I_{k}$
\end{proof}

\clearpage

\section{$K_{1}$}

In looking at $K_{1}$ we will move from looking at projective modules and their various properties and ways of classifying them to looking at maps between projective modules, their properties and ways of classifying them. As all these maps are matrices, we already have most of the tools required to do this. However, in a similar way as to how we had to look at projective modules before defining $K_{1}$, it is necessary here to look at some basic properties of the elementary matrices before moving on to the definition.

\subsection{Defintion of $K_{1}$}

\begin{defn}
Let $R$ be a ring. An elementary matrix lies in $GL(R)$ with 1's on the diagonal and precisely one non-zero, off diagonal entry. An elementary matrix with $a\in R$ in the $(i,j)$ position ($i\neq j$) is denoted $e(a)_{i,j}$. The group of matrices generated by these is $E(n,R)\subseteq GL(n,R)$. We can emned $E(n,R)$ in to $E(n+1,R)$ exactly as we did for $GL(n,R)$ and so define $\underset{n}{\bigcup} E(n,R)=E(R)\subseteq GL(R)$. When referring to the group of elementary matrices we shall be referring to this union.
\end{defn}

\begin{lem}
The following relations hold in $E(n,R)$ for $n\geq 3$ and in $E(R)$
\begin{enumerate}
 \item $e_{ij}(a)e_{ij}(b)=e_{ij}(a+b)$
 \item $e_{ij}(a)e_{kl}(b)=e_{kl}(b)e_{ij}(a+b)$ if $k\neq j$ and $l\neq i$
 \item $e_{ij}(a)e_{jk}(b)e_{ij}(a)^{-1}e_{jk}(b)^{-1}=e_{ik}(ab)$ if $i,j,k$ are distinct
 \item $e_{ij}(a)e_{ki}(b)e_{ij}(a)^{-1}e_{ki}(b)^{-1}=e_{ij}(-ba)$
\end{enumerate}
Also, any upper or lower triangular matrix with 1's on the diagonal is in $E(R)$.
\end{lem}
\begin{proof}
The matrix relations can easily be checked.

Suppose $A=(a_{i}$ is an uper triangular matrix with 1's on the diagonal. Then we canultiply to get a new matrix $$A`=Ae_{12}(-a_{12})e_{23}(a_{23})\ldots e_{n-1,n}(a_{n-1,n})$$ which will have 1's on the diagonal and 0's directly above the diagonal. We can continue to multiply till we get a matrix with 1's on the diagonal and just one non-zero entry in the $(1,n)$ position. So this matrix is in $E(n,R)$ and multiplying by the inverse of the elementary matrices used to gain this gives back $A$, so $A$ is a product of elementary matrices and hence in $E(R)$.
\end{proof}

\begin{cor}
For any $A\in GL(n,R)$ then $$\left(\begin{array}{cc} A & 0 \\ 0 & A^{-1} \end{array}\right)\in E(2n,R)$$
\end{cor}
\begin{proof}
Frist, note that $$\left(\begin{array}{cc} A & 0 \\ 0 & A^{-1} \end{array}\right)=
\left(\begin{array}{cc} 1 & A \\ 0 & 1 \end{array}\right)
\left(\begin{array}{cc} 1 & 0 \\ -A^{-1} & 1 \end{array}\right)
\left(\begin{array}{cc} 1 & A \\ 0 & 1 \end{array}\right)
\left(\begin{array}{cc} 0 & -1 \\ 1 & 0 \end{array}\right)$$.

By the previous lemma, the first three matrices on the right hand side lie in $E(2n,R)$ as the are upper or lower triangular.

Also $$\left(\begin{array}{cc} 1 & 0 \\ 0 & -1 \end{array}\right)=
\left(\begin{array}{cc} 1 & -1 \\ 0 & 1 \end{array}\right)
\left(\begin{array}{cc} 1 & 0 \\ 1 & 1 \end{array}\right)
\left(\begin{array}{cc} 1 & -1 \\ 0 & 1 \end{array}\right)$$

Again, by theprevious lemma the right hand side lies in $E(2n,R)$ and hence
$$\left(\begin{array}{cc} A & 0 \\ 0 & A^{-1} \end{array}\right)\in E(2n,R)$$
\end{proof}

\begin{thm}{Whitehead's Lemma}

For a ring $R$ the commutator subgroups of $GL(R)$ and $E(R)$ are equal to$E(R)$. So $E(R)\triangleleft GL(R)$ and $GL(R)/E(R)=GL(R)_{ab}$.
\end{thm}
\begin{proof}
First note that $E(R)\subseteq GL(R)$ $\Rightarrow$ $[E(R),E(R)]\subseteq [GL(R),GL(R)]$.

Now, for distinct $i,j,k$ then $e_{ij}(a)=[e_{ik}(a),e_{kj}(1)]$, so the generators of $E(R)$ are commutators of twoother generators, and hence $E(R)=[E(R),E(R)]$, so we need only show $E(R)\subseteq [GL(R),GL(R)]$.
For any $A,B\in GL(n,R)$ take their embeddings in $GL(2n,R)$. Then
$$\left(\begin{array}{cc} ABA^{-1}B^{-1} & 0 \\ 0 & 1 \end{array}\right)=
\left(\begin{array}{cc} AB & 0 \\ 0 & B^{-1}A^{-1} \end{array}\right)
\left(\begin{array}{cc} A^{-1} & 0 \\ 0 & A \end{array}\right)
\left(\begin{array}{cc} B^{-1} & 0 \\ 0 & B \end{array}\right)$$

By the previous corollary the right nad side is in $E(2n,R)$ and so $ABA^{-1}B^{-1}\in E(R)$.
\end{proof}

\begin{defn}{$K_{1}(R)$}

Let $R$be a ring witha unit. Then $K_{1}(R):= GL(R)/E(R)=GL(R)_{ab}$. This is a multiplicative group, with identity $e=[I]$. Note that whilst the obviosu way to define multiplication is by the elements in $GL(R)$ (i.e. $[A]\times [B]=[AB]$). Alternatively we can define $[A]\times [B]=[A\oplus B]$ where
$$A\oplus B= \left(\begin{array}{cc} AB & 0 \\ 0 & B^{-1}A^{-1} \end{array}\right)$$, and as
$$\left(\begin{array}{cc} A & 0 \\ 0 & B \end{array}\right)= \left(\begin{array}{cc} AB & 0 \\ 0 & 1 \end{array}\right)  \left(\begin{array}{cc} B^{-1} & 0 \\ 0 & B \end{array}\right)$$ we have $[A\oplus B=[AB\oplus I]=[AB]$. Note that $\left(\begin{array}{cc} B^{-1} & 0 \\ 0 & B \end{array}\right)\in E(2n,R)$

\end{defn}

Note that $K_{1}$ is a functor; if $\phi:RT\rightarrow S$ then by taking the induced map $GL(R)\rightarrow GL(S)$ and composing with the quotient map we get a map $K_{1}(R)\rightarrow K_{1}(S)$.

\textbf{insertex 2.1.6 about Cartesian product}

\subsection{Division rings and Local rings}

The easisest case for computing $K_{1}$ is in local and division rings, so these will be studied first. To do this we first need to look at the determinant fucntion.

\begin{prop}
Let $R$ be a commutative ring and $R*$ denote it's group of units. Then the function $det:GL(n,R)\rightarrow R*$ extends to a split surjection $GL(R)\rightarrow R*$ and so induces a split surjection $K_{1}(R)\rightarrow R*$.
\end{prop}
\begin{proof}
First note that $det(A\oplus I)=det(A)$, so if $A\in GL(n,R)$ is embedded in $GL(m,R)$ for some $m\geq n$ then the det function will be unchanged on that embedding. $det(AB)=det(A)det(B)$ and as $R$ is commutative then det must be a compostition of maps $GL(R)\rightarrow GL(R)_{ab}\rightarrow R*$.The splitting is defined by the natruall inclusion of $R*$ viewd as $GL(1,R)$ in to $GL(R)$.
\end{proof}

If we let $SL(R)$ denote the special linear matrices and notice that all matrices in $E(R)$ have determinant 1, so $E(R)\subseteq SL(R)$, then we can define $SK_{1}=SL(R)/E(R)\subseteq K_{1}(R)$

\begin{thm}
If $F$ is a field then $det:K_{1}(F)\rightarrow F*$ is an isomorphism and hence $SK_{1}(F)$ is trivial
\end{thm}
\begin{proof}
Let $A\in GL(n,F)$. Then let $a_{i1}$ be the first non-zero entry in the first column of $A$ and multiply $A$ by $e_{1i}(1)e_{i1}(-1)e_{1i}(1)$ to move it to the $a_{11}$ position. So assume that $a_{11}\neq 0$. Then to each row add $-a_{i1}a_{11}^{-1}$ so the first column is zero for all but hte first entry. Then this method can be continued to form an upper triangular matrix. all these operations are performed by elements of $E(R)$ so we can assume that our matrix is upper trianghular, as it will not effect the isomporphism class in $K_{1}(F)$.

Now that the matrix is in upper triangular form we can use $a_{11}$ to make all but the first entry in the top row zero. Similarly we can make the only non-zero entry in the $i^{th}$ row $a_{ii}$. Again this useonly elements on $E(n,F)$ and hence does nt affect the isomorphism class. Eventually we have a diagonal matrix with the same det as $A$.

Now by \textbf{corollary 3.1.3} we can see that a diagonal matrix with diagonal $(1,\ldots 1,a_{nn},a_{nn}^{-1},\ldots 1)\in E(n,F)$. And multplying by this in $E(F)$ will make the $an_{nn}$ entry a $1$. Continuing with this the matrix $A$ can be written as a diagonal matrix with $a_{11}$ being the only non-zero entry. Hence the determinant induces an isomorphism to $F*$. Now $a_{11}=det(A)$ and so $a_{11}=1$ iff $A\in SL(n,F)$ and if $a_{11}=1$ we can use elementary row operations to make $A=In$ and hance $A\in E(n,F)$ and $SL(n,F)=E(n,F)$.
\end{proof}

\begin{prop}
Let $R$ be a division ring. Then the inclusion $R*=GL(1,R)\hookrightarrow GL(R)$ induces a surjective map $R_{ab}*\rightarrow  K_{1}(R)$.
\end{prop}
\begin{proof}
As in the proof above  we can change any matrix in $GL(R)$ to a diagonal matrix with only the first entry possibly not $1$. So clearly $R*\twoheadrightarrow K_{1}(R)$. As $K_{1}$ is abelain then thsi map must factor through $R*_{ab}$
\end{proof}

\begin{prop}
If $R$ is a local ring then  the inclusion map $R*\hookrightarrow GL(R)$ induces a surjection from $R_{ab}*\twoheadrightarrow K_{1}(R)$
\end{prop}
\begin{proof}
The proof is almost identical to the one above, but as not all elements are invertible it is necessary to take the first invertible element (rather than non-zero element). This must exist in each column and row as otherwise $A$ would not be 
\end{proof}

\begin{defn}
Let $R$ be a local ring. Then define $det:GL(R)\rightarrow R_{ab}*$ to be a map which satisfies the following:
\begin{enumerate}
 \item If a matrix $A$ is transformed in to $\tilde{A}$ by adding the $i^{th}$ row to the $j^{th}$ then $det(A)=det(\tilde{A}$
 \item $det(I)=1$
 \item If, given a matrix $A$ we multiply one column on the left by some $a\in R*$ and the resultant matrix is $\tilde{A}$ then $det(A)=\bar{a}det(\tilde{A})$ where $\bar{a}=q(a)$ ($q$ the quotient map $R\rightarrow R_{ab}$)

\end{enumerate}

\end{defn}

\begin{thm}
The function $det:GL(R)\rightarrow R_{ab}*$ exists and is well defined, and also satisfies the following $\forall A,B\in GL(R)$  :
\begin{enumerate}
 \item $det(AB)=det(A)det(B)$
 \item If $\tilde{A}$ is $A$ with two rows swapped then $det(A)=-det(\tilde{A})$
 \item $det(A)=det(A^{T})$
\end{enumerate}
\end{thm}

\begin{proof}
Uniqueness: As already shown in previous proofs, any invertible matix over a local ring can be row reduced to a diagonal matrix $D=\begin{pmatrix} a & 0 & \cdots \\ 0 & 1 & \cdots \\ \vdots & \vdots & \ddots \end{pmatrix}$. By the first property of $det$, then $det(A)=det(D)$. By the third property $det(D)=\bar{a}det(I)$ and so by the second property $det(A)=det(D)=\bar{a}$.
\newline 1) Let $E\in E(R)$ be the matrix that represents the row operations to get $D$ as above. Then $EA=D$. By the first property of $det$. then $det(AB)=det(EAB)$, as $E$ still just represents row operations on $AB$. Also $det(AB)=det(EAB)=det(DB)$. Now multiplying by $D$ multiplies the top row of a matrix by $\bar{a}$ and leaves the rest unchanged, so $det(DB)=\bar{a}det(B)$ and 1) is proved.
\newline 2)It is a simple computation to show that if one wishes to change the $i^{th}$ and $j^{th}$ rows of a matrix in $GL(R)$ then one only needs to multiply by the elementary matrices $e_{ij}e_{ji}(-1)e_{ij}(1)$ and then multiply the $i^{th}$ row by $-1$. The first does not change determinant and the second  multiplies it by $-1$, hence proving 2)
\newline 3) Let $det':GL(R)\rightarrow R_{ab}*$ by $A\mapsto det(A^{T}$. Row operations on $A$ correspond to column operations on $A^{T}$, and column opreations are represented by matrices in $E(R)$,  but multiplied on the right not left. As $det(E)=1\ \forall E\in E(R)$ and $det(AB)=det(A)det(B)$ then column operations do not change $A$. Hence row operations on do not chane $det'$ and it satisfies the first condition of $det$. Also, $I^{T}=I$, hence it satisfies the second condition. lastly by a similar argument to the one above and noting that multiplying a row in $A^{T}$ coresponds to multiplying a column in $A$ then it satisfies the conditions of $det$ and so is uniquely determined.
Existence: Not proven here
\end{proof}

Note that the det function is consistent in $GL(n,R)$. By this I mean that it if $A\in GL(n,R)$ is embedded in $GL(m,R)$ for some $m>n$ then $det$ will give the same value in $R_{ab}*$ for both.

\begin{cor}
For any local ring $R$ then $K_{1}(R)\cong R_{ab}*$
\end{cor}
\begin{proof}
As already shown, there is a surjective map from $R_{ab}*\twoheadrightarrow K_{1}(R)$. Also it is clear that $det$ induces a surjection to $R_{ab}*$ from $K_{1}(R)$ as, by its definition, it is invariant on $E(R)$.
\end{proof}

\textbf{exercise 2.2.8?}

\subsection{PIDs and Dedekind Domains}

\begin{defn}
A commutative domain is Euclidean if there is a norm $||:R\rightarrow \mathbb{N}$ such that $\forall a,b\in R\ b\neq 0$ then
\begin{enumerate}
 \item $|a|=0$ iff $a=0$
 \item $|ab|=|a||b|$
 \item $\exists r,q\in R$ such that $a=qb+r$ and $0\leq |r|<|b|$
\end{enumerate}
\end{defn}


\begin{thm}
If $R$ is a Euclidean then $K_{1}(R)\cong R*$ and so $SK_{1}(R)$ is trival. This is equivalent to saying $SL(R)=E(R)$
\end{thm}
\begin{proof}
The idea of the proof is as in theorem 3.2.2. However, we do not necessarly have an invertible element. Let $A\in GL(R)$. To show that a 1 can be put in to the matrix by row and column operations. start with the $a_{i1}$ with lowest non-zero norm value.If $|a_{i1}|=1$ then $1=qa_{i1}$ for some $q$ and we are done.
\newline If not, then the ideal $(a_{i1})$ is proper. However, as $A$ is invertible then ideal generated by all the elements on the first row $R$; if $AB=I$ then $a_{11}b_{11}+\ldots +a_{n1}b_{n1}=1$. So choose some $a_{j1}\notin (a_{i1})$. We can write $a_{j1}=qa_{i1}+r$, so subtracting $q$ times the $j^{th}$ row from the $i^{th}$ row we can replace $a_{i1}$ by $r$, decreasing the norm. Re-applying this we must eventually get something with norm 1. From here proceed as in previous proofs.
\end{proof}

It turns out that this is the strongest result we can get for vanishing $SK_{1}(R)$ - if we have anything less than a Euclidean domain then $SK_{1}(R)$ is not guaranteed to be trivial. However, as already note, $SK_{1}(R)$ is trivial if and only if $E(R)=SL(R)$, which implies that $K_{1}(R)\cong GL(R)/SL(R)\cong R*$. The next strongest result that can be gained is for Dedekind domains, showing that if $R$ is Dedekind then $K_{1}(R)\cong GL(2,R)$. Before showing this a simple result is needed.

\begin{lem}
For any ring $R$ the quotient map $q:E(R)\rightarrow E(R/I)$ is surjective.
\end{lem}
\begin{proof}
Clearly any generator in $E(R/I)$ can be lifted to a matrix with non-zero off diagonal entry and 1's on the diagonal; ie a generator of $E(R)$
\end{proof}

\begin{thm}
Let $R$ be a Dedekind domain. Then $K_{1}(R)\cong GL(2,R)\subseteq GL(R)$
\end{thm}
\begin{proof}
Let $A\in GL(R)$ be the embedding of a matrix in $GL(n,R)$. Then we firstly need to show that this can row reduced to $\begin{pmatrix} 1 & v \\ 0 & A' \end{pmatrix}$ where $a'\in GL(n-1,R)$. Once this is done it can clearly be changed by elementary column operations to $\begin{pmatrix} 1 & 0 \\ 0 & A'' \end{pmatrix}$, which is in the same class in $K_{1}(R)$ as $a''\in GL(n-1,R)$.
\newline Noticing that as $A$ is invertible, a linear combination of $a_{11},\ldots ,a_{n1}$ is $1$ then if there is a zero in the $i^{th}$ entry in the first column, by adding a linear combination of rows this zero entry can be made a $1$. Then subtracting $a_{j1}$ lots of the $i^th$ row from the $j^{th}$ ($j\neq i$) we get $a_{j1}=0 \forall j\neq i$. Then pre-multiply by $e_{1i}(1)e_{i1}(-1)e_{i1}(1)$ and we have a matrix in the desired form. So it suffices to show that by using elementary row operations a zero can be made in the first column.
\newline Let $I=\rangle a_{31},\ldots ,a_{n1} \langle$. If $I=0$ then $a_{31}=)$ and we are done. If $I=R$ then we a linear combination of $a_{31},\ldots ,a_{n1}$ will equal $-a_{11}$, so adding that to the top row will mean we are done. Sothis leaves only the cases when $I$ is a proper ideal
\newline By theorem 2.5.9 we can factor $I$ uniquley in to prime ideals so that $I=P_{1}^{k_{1}}\ldots P_{m}^{k_{m}}$ , and then as in the proof of corollary 2.4.15 $R/I\cong R/P_{1}^{k_{1}}\times \ldots \times R/P_{m}^{k_{m}}$. Each $P_{i}$ is maximal, hence $R/P_{i}^{k_{i}}$ is a field and hence $SK_{1}(R/P_{i}^{k_{i}})=0$. Hence by thereom \textbf{inser number here when done} $SK_{1}(R/I)=0$, and so by definition pf $SK_{1}$ then $SL(m,R/I)=E(m,R/I)$.
\newline Let $\bar{a}$ denote the image in $R/I$ of $a\in R$. Then notice that $\bar{a_{11}}(R/I)+\bar{a_{21}}(R/I)=R/I$. Therefore $\exists \bar{c_{1}},\bar{c_{2}}\in R/I$ such that
$det\begin{pmatrix} \bar{b_{1}} & \bar{b_{2}} \\ -\bar{a_{12}} & \bar{a_{11}} \end{pmatrix}=1_{R/I}$. 
So this matrix lies in $SL(2,R/I)=E(2,R/I)$. By the previous lemma there is a matrix
$Q=\begin{pmatrix} d_{1} & d_{2} \\ -b_{12} & b_{11} \end{pmatrix}\in SL(2,R)$, 
where the quotient is equal to the previous one. Now $b_{1}a_{11}+b_{2}a_{12}-1\in I$, so $\exists b_{3}\ldots b_{n}\in I$ such that $b_{1}a_{11}+b_{2}a_{12}-1=b_{3}a_{13}+\ldots b_{n}a_{1n}$ or, alternatively, such that $\sum_{i=1}^{n}b_{i}a_{1i}=1$. Now rewriting 
\end{proof}

As $GL(2,R)\cong R*\oplus SL(2,R)$, the above theorem should have demonstrated the importance of studying $SK_{1}(R)$. An easy way to view this is using Mennicke symbols, which are defined below

\begin{defn}{Mennicke symbols}
If $A=\begin{pmatrix} a & b \\ c & d \end{pmatrix}\in SL(2,R)$ then the Mennicke symbol of is $[a\ b]=[A]\in SK_{1}(R)$.
\end{defn}

\begin{thm}
\begin{enumerate}
 \item For any $A=\begin{pmatrix} a & b \\ c & d \end{pmatrix}\in SL(2,R)$ and $B=\begin{pmatrix} a & b \\ c' & d' \end{pmatrix}\in SL(2,R)$ then $[A]=[B]\in SK_{1}(R)$, hence the Mennicke symbol is well defined. Also, by the last theorem, all elements of $SK_{1}(R)$ can be written as Mennicke symbols whenever $R$ is a Dedekind domain.
 \item If $a\in R*$ then $[a\ b]=1$
 \item If $Rac+Rb=R$ then $[a\ b]\cdot [c\ b]=[ac\ b]$
 \item If $a,b\in R$ $a,b$ coprime, then $[a\ b]=[b\ a]$ and $\forall c\in R$ then $[a\ b]=[a+cb\ b]$.
\end{enumerate}
\end{thm}
\begin{proof}
\begin{enumerate}
 \item Notice that if $A=\begin{pmatrix} a & b \\ c & d \end{pmatrix}, B=\begin{pmatrix} a & b \\ c' & d' \end{pmatrix}\in SL(2,R)$ then $A=\begin{pmatrix} 1 & 0 \\ cd'-c'd & 1 \end{pmatrix}B$; as $\begin{pmatrix} 1 & 0 \\ cd'-c'd & 1 \end{pmatrix}\in E(2,R)$ then $[A]=[B]$ in $K_{1}(R)$. Lastly, as the Mennicke symbols can therefore represent the whole of $SL(2,R)$ in $SK_{1}(R)$, and as $K_{1}(R)\cong R*\oplus SL(2,R)$ for $R$ Dedkind, then they can represent the whole of $SK_{1}(R)$ for $R$ Dedekind.

 \item If $a\in R*$ then $\exists a^{-1}\in R$ and taking $a^{-1}c$ times the first row from the second gives $\begin{pmatrix} a & b \\ 0 & d-ca^{-1}b \end{pmatrix}=\begin{pmatrix} a & b \\ 0 & a^{-1} \end{pmatrix}$. Then premultiply by $\begin{pmatrix} a & 0 \\ 0 & a^{-1} \end{pmatrix}$ gives a strictly upper triangular, and hence elementary matrix.
 
\item Take the matrices $\begin{pmatrix} a_{1} & b \\ c_{1} & d_{1}\end{pmatrix}$
and $\begin{pmatrix} a_{2} & b \\ c_{2} & d_{2}\end{pmatrix}$.
Then notice that $$\begin{pmatrix} a_{1} & b & 0 \\ c_{1} & d_{1}& 0\\ 0 & 0 & 1\end{pmatrix}
\begin{pmatrix} a_{2} & 0 & b \\ 0 & 1 & 0\\ c_{2} & 0 & d_{2}\end{pmatrix}
\begin{pmatrix} 1& 0 & 0 \\ 0 & 1 & -a_{1} \\ 0 & 0 & 1\end{pmatrix}$$
$$=\begin{pmatrix} a_{1}a_{2} & b & a_{1}b \\ c_{1}a_{2} & d_{1} & c_{1}b \\ c_{2} & 0 & d_{2} \end{pmatrix}
\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & -a_{1} \\ 0 & 0 & 1 \end{pmatrix}$$
$$=\begin{pmatrix} a_{1}a_{2} & b & 0 \\ c_{1}a_{2} & d_{1} & -1 \\ c_{2} & 0 & d_{2}\end{pmatrix}$$

Finally, as $$\begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & -d_{2} \\ 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 1 & 0 \end{pmatrix} \begin{pmatrix} a_{1}a_{2} & b & 0 \\ c_{1}a_{2} & d_{1} & -1 \\ c_{2} & 0 & d_{2}\end{pmatrix} \begin{pmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ c_{2}a_{2} & d_{1} & 1 \end{pmatrix}$$
$$=\begin{pmatrix} a_{1}a_{2} & b & 0 \\ c_{2}-c_{1}a_{2}d_{2} & d_{1}d_{2} & 0 \\ 0 & 0 & 1 \end{pmatrix}$$
It is clear that $[a_{1}\ b][a_{2}\ b]=[a_{1}a_{2}\ b]$
\item As $\begin{pmatrix} a & b \\ c & d \end{pmatrix}\begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}=\begin{pmatrix} -b & a \\ -d & c \end{pmatrix}$ it follows that $[a\ b]=[-b\ a]$. Using this, 2) and 3) it is clear that $[a\ b]=[-b\ a]=[b\ a][-1\ a]=[b\ a]\cdot 1$

Finally, $\begin{pmatrix} 1 & 0 \\ k & 1 \end{pmatrix}\in E(R)$ and hence $$\begin{pmatrix} a & b \\ c & d \end{pmatrix} \begin{pmatrix} 1 & 0 \\ k & 1 \end{pmatrix} = \begin{pmatrix} a+bk & b \\ c+dk & d \end{pmatrix}$$ implies that $[a\ b]=[a+bk\ b]$.
\end{enumerate}
\end{proof}

\begin{cor}
If $R$ is Dedekind and for all non-trivial prime ideals $P$ of $R$ $R/P$ is finte then $SK_{1}(R)$ is a torsion group.
\end{cor}
\begin{proof}
We can consider all the Mennicke symbols as they span the whoe of $SK_{1}(R)$ for $R$ Dedekind. If $b=0$ then $a\in R*$, hence $[a\ b]=1$, and if $b\in R8$ then $[a\ b]=1$. So it is only necessary to look at non-zero, non-unital $b$.
In this case, the ideal $(b)\subsetneq R$, and so it can be factorised in to prime ideals; $(b)=P_{1}^{k_{1}}\ldots P_{n}^{k_{n}}$. For each $i$ $R/P_{i}^{n_{i}}$ is finite, hence so is $R/(b)$. Now,a s $\bar{a}\in R/(b)$, the image of $a$ in $R/(b)$ and as the units in this field are finite, there must exist $k\in \mathbb{N}$ such that $a^{k}\equiv 1\ mod(b)$; i.e. $\exists r\in R$ such that $a^{k}=1+rb$. So we now have $[a\ b]^{k}=[a^{k}\ b]=[1+rb\ b]=[1\ b]=1$.
Hence every element of $SK_{1}(R)$ is of finte order and $SK_{1}(R)$ is a torsion group.
\end{proof}


\subsection{Relative $K_{1}$ and the exact equence}

\begin{defn}
If $R$ is a unital ring and $I$ a two sided ideal of $R$ then the relative $K_{1}$ group of$R$ and $I$ is $K_{1}(R,I):=ker((p_{1})_{*}:K_{1}D(R,I)\rightarrow K_{1}(R))$.
\end{defn}

This defnition is obviosuly analagous to the defintiion for the relative $K_{0}$ group; however, it is possible to give an equivalent defnition that is analgous to the defintion of $K_{1}$. To dothis we need to define an analagous $GL(R,I)$ and $E(R,I)$.

\begin{defn}
Let $R$ be a ring and $I$ an ideal as in the defintion of relative $K_{1}$. Then $GL(R,I):=ker(q)$ where $q:GL(R)\rightarrow GL(R/I)$ is the quotient map. In a similar vein, $E(R,I)$ is defined to be the smallest normal subgroup of $E(R)$ generated by $\{e_{ij}(a)\}_{a\in I}$. Note that clearly $E(R,I)\subseteq GL(R,I)$.
\end{defn}

\begin{thm}
Let $R$ be a unital ring and $I\triangleright R$. Then $E(R,I)$ is a normal subgroup of $GL(R,I)$ and $$GL(R,I)/E(R,I)\cong K_{1}(R,I)$$
This is the centre of $GL(R)/E(R,I)$ and $E(R,I)=[E(R),E(R,I)]=[GL(R),E(R,I)$
\end{thm}
\begin{proof}
Let $A\in GL(n,R)$ and $B\in E(n,R,I)$. Then we have that $$\begin{pmatrix} ABA^{-1} & 0 \\ 0 & 1 \end{pmatrix} = \begin{pmatrix} A & 0 \\ 0 & A^{-1} \end{pmatrix} \begin{pmatrix} B & 0 \\ 0 & 1 \end{pmatrix} \begin{pmatrix} A^{-1} & 0 \\ 0 & A \end{pmatrix}$$
We already know that $\begin{pmatrix} A & 0 \\ 0 & A^{-1} \end{pmatrix}$ is elementary, and as $E(R,I)$ is defined to be normal the right hand side must lie in $E(R,I)$. Hence it is normal in $GL(n,R)$.
\newline Let $(A_{1},A_{2})\in GL(D(R,I))$ and in $K_{1}(R,I)$; ie $(p_{1})_{*}(A_{1},A_{2})=id_{K_{1}(R)}$. Therefore $A_{1}\in E(R)$ and hence $(A_{1},A_{1})\in E(D(R,I))$. Taking $(A_{1},A_{2})\cdot (A_{1},A{1})=(1,B)$ for some $B\in GL(R)$. though$(A_{1},A_{2})$ and $(1,B)$ have the same class in $K_{1}(R,I)$. So $B\equiv 1\ mod\ I$ so $B\in GL(R,I)$. Clearly all $B\in GL(R,I)$ give a class in $GL(D(R,I))$. So if we show that $B\in E(R,I)$ if and only if $(1,B)\in E(D(R,I))$ $\forall B\in GL(R,I)$ then we will have the desired isomorphism.
\newline The generators of $E(R,I)$ are $Se_{ij}(a)S^{-1}$, $S\in E(R)$ and $a\in I$, as $E(R,I)$ is normal and generated by $e_{ij}(a)$. But $(1,Se_{ij}(a)S^{-1})=(S,S)e_{ij}(0,a)(S^{-1},S^{-1})$. But the right hand side all lies in $E(D(R,I))$,so $B\in E(R)$ implies $B\in E(D(R,I))$.
\newline If $(1,B)=\prod_{k=1}^{n}e_{i_{k}j_{k}}(a_{k},b_{k})\in E(D(R,I))$ where $\prod_{k=1}^{n}e_{i_{k}j_{k}}(a_{k})=1$.
Now for each $k$ we can rewrite $e_{i_{k}j_{k}}(a_{k},b_{k})$ as $e_{i_{k}j_{k}}(a_{k},a_{k})e_{i_{k}j_{k}}(0,b_{k}-a_{k})=(S_{k},S_{k})(1,T_{k})$ where $S_{k}=e_{i_{k}j_{k}}(a_{k})$ and $T_{k}=e_{i{k}j_{k}}(b_{k}-a_{k})$. Notice that as $(1,B)\in E(D(R,I))$ then $a_{k}-b_{k}\in I$.So we can now write $$(1,B)=\prod_{k=1}^{n}e_{i_{k}j_{k}}(a_{k},b_{k})=\prod_{k=1}^{n}(S_{k},S_{k}T_{k})$$ $$=(S_{1},S_{1}T_{1}S_{1}^{-1})(S_{2},S_{1}S_{2}T_{2}S_{1}^{-1}S_{2}^{-1}\ldots (S_{n},S_{1}\ldots S_{n}T_{n})$$ $$=(1,(S_{1}T_{1}S_{1}^{-1})\ldots (S_{1}\ldots S_{n-1}T_{n-1}S_{n-1}^{-1}\ldots S_{1}^{-1})(T_{n}))$$. The last equality is gained as $S_{1}\ldots S_{n}=1$. Hence $B$ can be written as a product of elements of $E(R,I)$
\newline It has already been shown that $E(R,I)$ is normal in $GL(R)$ and $GL(R,I)$, and so it follows that $[E(R),E(R,I)]\subseteq [GL(R),E(R,I)]\subseteq E(R,I)$. It can be shown that they are equal as any gnereator can be written $Se_{ij}(a)S^{-1}=[S,e_{ij}(a)]e_{ij}(a)=[S,e_{ij}(a)][e_{ik}(1),e_{kj}(a)]\in [E(R),E(R,I)]$ for some $a\in I,\ S\in E(R)$ and $k\neq i,j$.
\newline Lastly, it remains toshow that $GL(R,I)/E(R,I)$ is the centre of $GL(R)/E(R,I)$. FIirstly, use the identity for any $A\in GL(R,I)$ that $$\begin{pmatrix} A & 0 \\ 0 & a^{-1}\end{pmatrix}=\begin{pmatrix} 1 & A-1 \\ 0 & 1 \end{pmatrix}  \begin{pmatrix} 1 & 0 \\ 1 & 1\end{pmatrix} \begin{pmatrix} 1 & -A^{-1}(A-1) \\ 0 & 1 \end{pmatrix} \begin{pmatrix} 1 & 0 \\ -1 & 1\ \end{pmatrix} \begin{pmatrix} 1 & 0 \\ -(A-1) & 1 \end{pmatrix}$$. As all entries of $A-1$ are in $I$, as shown ealier, the whole right hand side is in $I$, and so the left hand side is too.There fore if $B\in GL(R)$ then $\begin{pmatrix} ABA^{-1}B^{-1} & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{pmatrix}= [\begin{pmatrix} A & 0 & 0 \\ 0 & A^{-1} & 0 \\ 0 & 0 & 1 \end{pmatrix} \begin{pmatrix} B & 0 & 0 \\ 0 1 & 0 \\ 0 & 0 & B^{-1} \end{pmatrix}]$. So $GL(R)$ and hence $GL(R,I)$  commute modulo $E(R,I)$. However the quotient map must take the centre of $GL(R)/E(R,I)$ to the centre of $GL(R/I)$. Howeverm this is trivial, as a commutative matrix must be diagonal with all entries on the diagobal equal. However, there must be a 1 eventually on the diagonal in $GL(R)$, and so it is only the identity. Therefore the centre of $GL(R)/E(R,I)$ must be in the kernel of the quotient map, which is $GL(R,I)/E(R,I)$.
\end{proof}

It is now possible to extend the exact sequence from the first section with the relative $K_{0}$ to one that is of length six and includes $K_{1}$.

\begin{thm}
Let $R$ be a ring and $I\triangleleft R$ an ideal. Then there exists a natural exact sequence $K_{1}(R,I)\rightarrow K_{1}(R)\stackrel{q_{*}}{\rightarrow} K_{1}(R/I)\stackrel{\delta}{\rightarrow} K_{0}(R,I)\rightarrow K_{0}(R)\stackrel{q_{*}}{\rightarrow} K_{0}(R/I)$, in which $q_{*}$ is the map induced by the quotient map and the maps from $K_{i}(R,I)\rightarrow K_{i}(R)$ is induced by $p_{2}:D(R,I)\rightarrow R$.
\end{thm}
\begin{proof}
Let $\bar{A}:=q(A)$ in this proof.

The first thing that shall be shown is the exactness of $K_{1}(R,I)\rightarrow K_{1}(R)\stackrel{q_{*}}{\rightarrow} K_{1}(R/I)$. In the previous theorem it was shown that any class in $K_{1}(R,I)$ can be represented by some $(1,B)\in GL(D(R,I))$ for some $B\in GL(R,I)$ and so $q(B)=I_{R/I}$ and $q_{*}[B]=1$. Conversely, if $q_{*}[B]=1$ and $B\in GL(R)$ then $\bar{B}\in E(R/I)$. As $E(R/I)=q(E(R))$ then any matrix $\bar{B}\in E(R/I)$ can be lifted to a matrix $C\in E(R)$, and we also know that $q(BC^{-1})=1$. So $(1,BC^{-1})\in GL(D(R,I))$ and as $[(1,BC^{-1})]\in K_{1}(R,I)$ then $[B]=[BC^{-1}]$.
\newline Now it is necessary to define the map $\delta:K_{1}(R/I)\rightarrow K_{0}(R,I)$. TO do this we need to define $R^{n}\times_{\bar{A}} R^{n}$ for some $\bar{A}\in GL(n,R/I)$. It is defined as $\{(x,y)\in R^{n}\times R^{n}|\bar{y}=\bar{x}\bar{A}\}$. This can be made in to a $D(R,I)$ module by defining $(r_{1},r_{2})\cdot(x,y)=(r_{1}x,r_{2}y)$. THis is well defined as $\bar{r}_{1}=\bar{r}_{2}$ and so $\bar{r}_{2}\bar{y}=\bar{r}_{1}(\bar{x}\bar{A})$. Also, the map $(x,y)\mapsto (xA,y)\in R^{n}\times_{I} R^{n}\cong D(R,I)^{n}$ is an isomorphism, making $R^{n}\times_{\bar{A}} R^{n}$ free of rank $n$.Hence it is always projective, so we can define $\delta [\bar{A}]:=[R^{n}\times_{\bar{A}} R^{n}]-[D(R,I)^{n}]\in K_{0}(D(R,I))$. It's image is $K_{0}(R,I)$ as $(p_{1})_{*}([R^{n}\times_{\bar{A}} R^{n}]-(p_{1})_{*}([D(R,I]))=[R^{n}]-[R^{n}]=0$. Also, if $\bar{A}$ is elementary then $\delta [\bar{A}]=[R^{n}\times_{\bar{A}} R^{n}]=[D(R,I)^{n}]\cong [D(R,I)^{n}]-[D(R,I)^{n}]=0$. It is a homomorphism as $\delta ([\bar{A}])\oplus \delta ([\bar{B}])=[R^{n}\times_{\bar{A}} R^{n}]\oplus [R^{n}\times_{\bar{B}} R^{n}]=[R^{n}\times_{\bar{A}\oplus\bar{B}} R^{n}]=\delta (\bar{A}\oplus \bar{B})$. Lastly, it is well defined on equivalence classes in $K_{1}$ as if $\bar{A}=\bar{B}\bar{C}$ with $B\in E(R)$ then $(x,y)\mapsto xB,Y)$ is an isomorphism.
\newline It has already been shown that $im(q_{*})\supseteq ker(\delta )$. It is easy to see that $im(\delta )\supseteq ker((p_{2})_{*})$ as $(p_{2})_{*}(\delta [\bar{A}])=(p_{2})_{*}([R^{n}\times_{\bar{A}} R^{n}])-(p_{2})_{*}([D(R,I)^{n}])=[R^{n}]-[R^{n}]=0$.
\newline Let $\delta ([\bar{A}])=0$. So this means it is stably isomorphic to a free module, or in other words $\exists m,n\in mathbb{N}$ such that $R^{n}\times_{\bar{A}} R^{n}\oplus D(R,I)^{m}\cong D(R,I)^{n+m}$. Then we can replace $A$ by $A\oplus I_{m}$ to get $[R^{n}\times_{\bar{A}} R^{n}]\stackrel{\phi}{\cong} D(R,I)^{n}$. Denoting the standard basis for $R^{n}$ by $e_{i}$ there exist $B,C\in M_{n}(R)$ such that $\phi(e_{i},e_{i})=(e_{i}B,e_{i}C)$. So $\forall u,v\in D(R,I)^{n}$ we have $\phi(u,v)=(uB,vC)$ and as $\bar{u}=\bar{v}$ we have that $\bar{B}\bar{A}=\bar{C}$. As $\phi$ has inverse (being an isomorphism) then $B$ and$C$ are invertible, and so $\bar{A}=q(B^{-1}C)$, hence $ker(\delta )\subseteq im(q_{*})$.
\newline Lastly, it is necessary to show that $ker(p_{2})_{*}\subseteq \delta (K_{1}(R/I))$. Suppose we have an element of the kernel; write it $[P]-[D(R,I))^{n}]$. Then $(p_{1})_{*}(P)$ and $(p_{2})_{*}(P)$ are stably isomorphic to $R^{n}$. Then we can add on a free module of rank $k$ to $P$ and so $(p_{1})_{*}(P)$ and $(p_{2})_{*}(P)$ are both isomporphic to $R^{n+k}$; hence $P$ is clearly of the form $R^{n}\times_{\bar{A}} R^{n}$ meaning that $[P]-[D(R.I)^{n}]=\delta ([\bar{A}])$, thus finishing the proof
\end{proof}

\begin{cor}
Let $R$ be a ring and $I$ an idel of $R$ such that the quotient map $q:R\rightarrow R/I$ is the right inverse for soem injective map $s:R/I\hookrightarrow R$. Then the sequence $0\rightarrow K_{0}(I)\rightarrow K_{0}(R)\rightarrow K_{0}(R/I)\rightarrow 0$ is split exact.
\end{cor}
\begin{proof}
$s_{*}$ is a splitting map for $q_{*}$ by the functorality of $K_{0}$. It remains to show the injectivity of the map $K_{0}(I)\rightarrow K_{0}(R)$, but this follows by noticing that in the exact sequence $s_{*}:K_{1}(R/I)\rightarrow K_{1}(R)$ is splitting for $q_{}$, so $\delta =0$, and the result follows.
\end{proof}

\begin{lem}{Rim's lemma}

Let $G$ be a cyclic group of orer $p$, for some od prime $p$ and let $R=\mathbb{Z}G$.Then $R\cong \mathbb{Z}[t]/(t^{p}-1)$. Let $I$ be an ideal of $R$ defined to be kernel of the map $h:R\rightarrow \mathbb{Z}[\epsilon]$, where $\epsilon = e^{2\pi i/p}$, $h:t\mapsto \epsilon$. Then the boundary map $\delta :\mathbb{Z}^{\frac{p-3}{2}}\rightarrow \mathbb{F}_{p}*/\{\pm 1\}$ is surjective.
\end{lem}
\begin{proof}
\textbf{insert proof later}
\end{proof}

\begin{prop}
Let $R$ be a commutative ring, $I$ an ideal of $R$. Then $K_{1}(R,I)$ splits as $\{a\in R*|a\equiv \ mod\ I\}\times SK_{1}(R,I)$, where we define $SK_{1}(R)$ to be $SL(R,I)/E(R,I)$ and $SL(R,I)$ is defined to be $SL(R)\cap GL(R,I)$
\end{prop}
\begin{proof}
$det:GL(R,I)\twoheadrightarrow \{a\in R*|a\equiv \ mod\ I\}$ and clearly has kernel equal to $SL(R,I)$. To finish, quotient by $E(R,I)$ and apply theorem 3.4.3.
\end{proof}

\begin{cor}
Let $R$ be a commutatie ring, $I$ an ideal. Then the exact sequence of length six can be split in to two exact sequences:
$$1\rightarrow \{a\in R*|a\equiv \ mod\ I\}\rightarrow R* \rightarrow (R/I)*$$ and
$$SK_{1}(R,I)\rightarrow SK_{1}(R) \rightarrow SK_{1}(R/I)$$
\end{cor}
\begin{proof}
mmediate from the previous proposition.
\end{proof}

\begin{thm}
Let $R$ be Dedekind and $I$ an ideal of $R$. Then $SK_{1}(R,I)\cong SL(R,I)\oplus SL(2,R,I)$
\end{thm}
\begin{proof}

\end{proof}

\begin{thm}
Let $R$ be a commutative ring and $I$ an ideal of $R$. Then:
\begin{enumerate}
 \item Suppose $a,b\in R$ and $Ra+Rb=R,\ a\equiv 1 mod\ I,\ b\in I$. Then we can choose $c,d,\in R$ such that $c\in I$, $d\equiv 1\ mod\ I$ and $ad-bc\equiv 1\ mod \ I$. For such $a,b,c,d$ then the class of $\begin{pmatrix} a & b \\ c & d \end{pmatrix}\in SL(2,R,I)$ in $SK_{1}(R,I)$ is uniquely determined by $a,b$, and so can be written $[a\ b]_{I}$. This is a relative Mennicke symbol. If $R$ is Dedekind then all elements can be written as this.
 \item If $a\in R*,a\equiv 1\ mod\ I$ and $b\in I$ then $[a\ b]_{I}=1$.
 \item If $a,b\in R$ and they are coprime, with $a\equiv 1\mod \ I,b\in I$ then $[a\ b]_{I}=[a+pb\ b]_{I}$ for any $p\in R$ and for any $q\in I$ then $[a\ b]_{I}=[a\ b+qa]_{I}$
 \item If $a,b\in R$ and they are coprime, with $a\equiv 1\mod \ I,b\in I$ and $b\equiv \pm a\ mod\ I$ then $[a\ b]_{I}=1$
 \item If both sides are defined than $[a\ b_{1}]_{I}\cdot [a\ b_{2}]_{I}=[a\ b_{1}b_{2}]_{I}$
\end{enumerate}
\end{thm}

\begin{cor}
If $R$ is Dedekind nad $R/P$ is finte for all non-zero prime ideals $P$, then $I$ being a proper ideal of $R$ implies that $SK_{1}(R,I)$ is a trosion group.
\end{cor}

\clearpage

\section{$K_{2}$}

\subsection{Universal central extension and $H_{2}$}

Before being able to define $K_{2}$ of a ring it is necessary to introduce some group theory and homology concepts.

\begin{defn}{Central extension}

Let $G$ be a group, $A$ an abelian group. A central extension of $G$ by $A$ is a pair $(E,\phi)$, where $E$ is a group conating $A$ as a central subgroup and $\phi:E\twoheadrightarrow G$ is surjective homorphism whose kernel is precisly $A$. This is equivalent to saying that the followinf is a short exact sequence $1\rightarrow A \rightarrow E \stackrel{\phi}{\rightarrow} G \rightarrow 1$ and that $A$ is central in $E$.
\end{defn}

\begin{defn}
Let $(E,\phi)$ and $(E',\phi')$ be central extensions of the same group $G$. A morphism is a map $\psi:(E,\phi)\rightarrow (E',\phi')$ satisfying the follwoing commutative diagram:
\textbf{insert commutative diagram}
Hence central extensions form a category.
\end{defn}

\begin{defn}
A central extension of $G$ by $A$ is trivial if it is isomorphic to $(G\times A,p_{1})$ where $p_{1}$ is the projection of the first co-ordinate.
\newline A central extension $(E,\phi)$is universal if for any other central extension $(E',\phi')$ of $G$ $\exists !$ isomorphism $\psi:(E,\phi)\rightarrow (E',\phi')$. If a universal extension exists then it is unique up to isomorphism.
\end{defn}

\begin{defn}
A group $G$ is prefect if $G=[G,G]$.
\end{defn}

\begin{thm}
A group has a universal central extension if and only if it isperfect. If a group $G$ is perect then a central extension $(E,\phi)$ is universal if and only if it satisfies the following:
\begin{enumerate}
 \item $E$ is perfect
 \item All central extensions of $E$ are trivial
\end{enumerate}
\end{thm}

Confirming whether or not a group is perfect is relatively straightforward in most cases. However, it is rarely easy to check whether or not a central extension satisfies the second condition directly. However, once homology groups are introduced later in the chapter then it will be easy to check whether or not the second homology group is zero, and this will be shown to be equivalent to all central extensions being trivial.

Howver, even if only the first condition holds it can be shown that then $(E,\phi)$ will be a quotient of the universal extension.
\textbf{include more}

\textbf{example}

\begin{defn}
Let $G$ be a group. A left G-module $M$ is an abelian group with a left action $G\times M\mapsto M$ such tht $g\cdot (h\cdot m)=(g\cdot h)\cdot m)$ and $1\cdot m=m$ for any $g,h\in G$, $m\in M$. This is equivalent to making it a left $\mathbb{Z}G$ module, where $\mathbb{Z}G$ is the group ring of $G$.
\end{defn}

\begin{defn}
Let $G$ be a group, $M$ a G-module. We can now go on to define the homology and cohomology  groups of $G$ with co-efficients in $M$ by starting with constructing a free resolution of $\mathbb{Z}$ (an exact sequence of free modules ending at $\mathbb{Z}$). Let $P_{j}=\mathbb{Z}G^{j+1}$ for $j\geq 0$ where $G^{j+1}$ is the cartesian product of $G$ with itself $j+1$ times. Then the map $d_{j}:P_{j}\rightarrow P_{j-1}$ is defined as follows: $$d_{j}(g_{0},\ldots ,g_{j})=\sum_{k=0}^{j}(-1)^{k}(g_{0},\ldots ,\hat{g}_{k},\ldots g_{j})$$, where $\hat{g}_{k}$ means remove the $k^{th}$ element.The map $\epsilon: P_{0}\rightarrow \mathbb{Z}$ is defined by $g\mapsto 1$. Then the sequence $$\ldots \stackrel{d_{j+1}}{\rightarrow} P_{j} \stackrel{d_{j}}{\rightarrow} \ldots \stackrel{d_{1}}{\rightarrow} P_{0} \stackrel{\epsilon}{\rightarrow} \mathbb{Z} \rightarrow 0$$ is exact.

Notice that the boundary maps are left and right G-module maps for the diagonal action. 

Then the homology group of $G$ with co-efficents in $M$ are denoted $C_{n}(G,M)$ and defined $$C_{n}(G,M)=P_{n}\otimes_{\mathbb{Z}G}M\cong \mathbb{Z}G^{n}\otimes_{\mathbb{Z}}M$$

The cohomology group of $G$ with coefficients in $M$ is are denoted $C^{n}(G,M)$ and defined $$C^{n}(G,M)=Hom_{\mathbb{Z}G}(P_{n},M)\cong Hom_{\mathbb{Z}}(\mathbb{Z}G^{n},M)$$
\end{defn}

\textbf{insert remarks here}

\begin{prop}
Let $G$ be a group. Then for each $k\geq 0$, $M\rightsquigarrow H_{k}(G,M)$ and $M\rightsquigarrow H^{k}(G,M)$ are covariant functors.
\newline If $M$ is projective or injective (\textbf{define at the beginning of project}) then $H_{k}(G,M)=0$ $\forall k>0$.
\newline Also, if $M_{1},m_{2}$ and $M_{3}$ are G-modules and the sequence $$0{\rightarrow} M_{1}\stackrel{\alpha}{\rightarrow} M_{2} \stackrel{\beta}{\rightarrow} M_{3} \rightarrow 0$$ is short exact then there is a long exact sequence $$\ldots \stackrel{\beta_{*}}{\rightarrow} H_{k+1}(G,M_{3})\stackrel{\delta}{\rightarrow} H_{k}(G,M_{1})\stackrel{\alpha_{*}}{\rightarrow} H_{k}(G,M_{2}) \stackrel{\beta_{*}}{\rightarrow} H_{k}(G,M_{3})\stackrel{\delta}{\rightarrow} H_{k-1}(G,M_{1}) \stackrel{\alpha_{*}}{\rightarrow} \ldots$$
and similarly for cohomology.
\end{prop}



\end{document}
